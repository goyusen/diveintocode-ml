{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "inner-interface",
   "metadata": {},
   "source": [
    "# Sprint 自然言語処理入門"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-franklin",
   "metadata": {},
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "**<u>Sprintの目的</u>**\n",
    "\n",
    "- 自然言語処理の一連の流れを学ぶ\n",
    "- 自然言語のベクトル化の方法を学ぶ\n",
    "\n",
    "**<u>どのように学ぶか</u>**\n",
    "\n",
    "自然言語処理定番のデータセットを用いて、一連の流れを見ていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-creator",
   "metadata": {},
   "source": [
    "## 2.自然言語のベクトル化\n",
    "\n",
    "**自然言語処理（NLP, Natural Language Processing）** とは人間が普段使っている **自然言語** をコンピュータに処理させる技術のことです。ここではその中でも、機械学習の入力として自然言語を用いることを考えていきます。\n",
    "\n",
    "\n",
    "多くの機械学習手法は **数値データ（量的変数）** の入力を前提にしていますので、自然言語の **テキストデータ** を数値データに変換する必要があります。これを **自然言語のベクトル化** と呼びます。ベクトル化の際にテキストデータの特徴をうまく捉えられるよう、様々な手法が考えられてきていますので、このSprintではそれらを学びます。\n",
    "\n",
    "**<u>非構造化データ</u>**\n",
    "\n",
    "データの分類として、表に数値がまとめられたようなコンピュータが扱いやすい形を **構造化データ** 、人間が扱いやすい画像・動画・テキスト・音声などを **非構造化データ** と呼ぶことがあります。自然言語のベクトル化は、非構造化データを構造化データに変換する工程と言えます。同じ非構造化データでも、画像に対してはディープラーニングを用いる場合この変換作業はあまり必要がありませんでしたが、テキストにおいてはこれをどう行うかが重要です。\n",
    "\n",
    "\n",
    "**<u>自然言語処理により何ができるか</u>**\n",
    "\n",
    "機械学習の入力や出力に自然言語のテキストを用いることで様々なことができます。入力も出力もテキストである例としては **機械翻訳** があげられ、実用化されています。入力は画像で出力がテキストである **画像キャプション生成** やその逆の文章からの画像生成も研究が進んでいます。\n",
    "\n",
    "\n",
    "しかし、出力をテキストや画像のような非構造化データとすることは難易度が高いです。比較的簡単にできることとしては、入力をテキスト、出力をカテゴリーとする **テキスト分類** です。\n",
    "\n",
    "\n",
    "アヤメやタイタニック、手書き数字のような定番の存在として、**IMDB映画レビューデータセット** の感情分析があります。レビューの文書が映画に対して肯定的か否定的かを2値分類します。文書ごとの肯定・否定はラベルが与えられています。このSprintではこれを使っていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-survey",
   "metadata": {},
   "source": [
    "## 3.IMDB映画レビューデータセットの準備\n",
    "\n",
    "IMDB映画レビューデータセットを準備します。\n",
    "\n",
    "\n",
    "**<u>ダウンロード</u>**\n",
    "\n",
    "次のwgetコマンドによってダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "herbal-negative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-07 14:52:32--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "ai.stanford.edu (ai.stanford.edu) をDNSに問いあわせています... 171.64.68.10\n",
      "ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 84125825 (80M) [application/x-gzip]\n",
      "`aclImdb_v1.tar.gz' に保存中\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  6.29MB/s 時間 13s        \n",
      "\n",
      "2021-04-07 14:52:45 (6.20 MB/s) - `aclImdb_v1.tar.gz' へ保存完了 [84125825/84125825]\n",
      "\n",
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n"
     ]
    }
   ],
   "source": [
    "# IMDBをカレントフォルダにダウンロード\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# 解凍\n",
    "!tar zxf aclImdb_v1.tar.gz\n",
    "# aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf aclImdb/train/unsup\n",
    "# IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-uncertainty",
   "metadata": {},
   "source": [
    "以下のサイトで公開されているデータセットです。\n",
    "\n",
    "[Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-ghost",
   "metadata": {},
   "source": [
    "**読み込み**\n",
    "\n",
    "scikit-learnのload_filesを用いて読み込みます。\n",
    "\n",
    "\n",
    "[sklearn.datasets.load_files — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spare-wales",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-stream",
   "metadata": {},
   "source": [
    "**<u>このデータセットについて</u>**\n",
    "\n",
    "中身を見てみると、英語の文章が入っていることが分かります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prerequisite-surface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n"
     ]
    }
   ],
   "source": [
    "print(\"x : {}\".format(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "arctic-mapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interracial-occupation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clichés, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\\'t list them here, but just mention the coloring of the plane. They didn\\'t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\\' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you\\'re choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "industrial-landing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-questionnaire",
   "metadata": {},
   "source": [
    "IMDBはInternet Movie Databaseの略で、映画のデータベースサイトです。\n",
    "\n",
    "\n",
    "[Ratings and Reviews for New Movies and TV Shows - IMDb](https://www.imdb.com/)\n",
    "\n",
    "\n",
    "このサイトではユーザが映画に対して1から10点の評価とコメントを投稿することができます。そのデータベースから訓練データは25000件、テストデータは25000件のデータセットを作成しています。\n",
    "\n",
    "\n",
    "4点以下を否定的、7点以下を肯定的なレビューとして2値のラベル付けしており、これにより感情の分類を行います。5,6点の中立的なレビューはデータセットに含んでいません。また、ラベルは訓練用・テスト用それぞれで均一に入っています。詳細はダウンロードしたREADMEを確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-cornwall",
   "metadata": {},
   "source": [
    "## 4.古典的な手法\n",
    "\n",
    "古典的ながら現在でも強力な手法であるBoWとTF-IDFを見ていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-banks",
   "metadata": {},
   "source": [
    "## 5.BoW\n",
    "\n",
    "単純ながら効果的な方法として **BoW (Bag of Words)** があります。これは、サンプルごとに単語などの **登場回数** を数えたものをベクトルとする方法です。単語をカテゴリとして捉え **one-hot表現** していることになります。\n",
    "\n",
    "\n",
    "<u>例</u>\n",
    "\n",
    "例として、IMDBデータセットからある3文の最初の5単語を抜き出したものを用意しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tight-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset = \\\n",
    "  [\"This movie is very good.\",\n",
    "  \"This film is a good\",\n",
    "  \"Very bad. Very, very bad.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-software",
   "metadata": {},
   "source": [
    "この3文にBoWを適用させてみます。scikit-learnのCountVectorizerを利用します。\n",
    "\n",
    "\n",
    "[sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "engaged-transmission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  is  movie  this  very\n",
       "0  0    0     0     1   1      1     1     1\n",
       "1  1    0     1     1   1      0     1     0\n",
       "2  0    2     0     0   0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "# DataFrameにまとめる\n",
    "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-edmonton",
   "metadata": {},
   "source": [
    "例にあげた3文の中で登場する8種類の単語が列名になり、0,1,2番目のサンプルでそれらが何回登場しているかを示しています。2番目のサンプル「Very bad. Very, very bad.」ではbadが2回、veryが3回登場しています。列名になっている言葉はデータセットが持つ **語彙** と呼びます。\n",
    "\n",
    "\n",
    "テキストはBoWにより各サンプルが語彙数の次元を持つ特徴量となり、機械学習モデルへ入力できるようになります。この時使用したテキスト全体のことを **コーパス** と呼びます。語彙はコーパスに含まれる言葉によって決まり、それを特徴量としてモデルの学習を行います。そのため、テストデータではじめて登場する語彙はベクトル化される際に無視されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-appendix",
   "metadata": {},
   "source": [
    "**<u>前処理</u>**\n",
    "\n",
    "CountVectorizerクラスでは大文字は小文字に揃えるという **前処理** が自動的に行われています。こういった前処理は自然言語処理において大切で、不要な記号などの消去（**テキストクリーニング**）や表記揺れの統一といったことを別途行うことが一般的です。\n",
    "\n",
    "\n",
    "語形が「see」「saw」「seen」のように変化する単語に対して語幹に揃える **ステミング** と呼ばれる処理を行うこともあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-burning",
   "metadata": {},
   "source": [
    "**<u>トークン</u>**\n",
    "\n",
    "BoWは厳密には単語を数えているのではなく、 **トークン（token）** として定めた固まりを数えます。\n",
    "\n",
    "\n",
    "何をトークンとするかはCountVectorizerでは引数token_patternで **正規表現** の記法により指定されます。デフォルトはr'(?u)\\b\\w\\w+\\b'ですが、上の例ではr'(?u)\\b\\w+\\b'としています。\n",
    "\n",
    "\n",
    "デフォルトでは空白・句読点・スラッシュなどに囲まれた2文字以上の文字を1つのトークンとして抜き出すようになっているため、「a」や「I」などがカウントされません。英語では1文字の単語は文章の特徴をあまり表さないため、除外されることもあります。しかし、上の例では1文字の単語もトークンとして抜き出すように引数を指定しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-experiment",
   "metadata": {},
   "source": [
    "**<u>《正規表現》</u>**\n",
    "\n",
    "\n",
    "正規表現は前処理の際にも活用しますが、ここでは詳細は扱いません。Pythonではreモジュールによって正規表現操作ができます。\n",
    "\n",
    "\n",
    "[re — 正規表現操作](https://docs.python.org/ja/3/library/re.html)\n",
    "\n",
    "\n",
    "正規表現を利用する際はリアルタイムで結果を確認できる以下のようなサービスが便利です。\n",
    "\n",
    "\n",
    "[Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript](https://regex101.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-composition",
   "metadata": {},
   "source": [
    "**<u>形態素解析<u/>**\n",
    "    \n",
    "英語などの多くの言語では空白という分かりやすい基準でトークン化が行えますが、日本語ではそれが行えません。\n",
    "\n",
    "\n",
    "日本語では名詞や助詞、動詞のように異なる **品詞** で分けられる単位で **分かち書き** することになります。例えば「私はプログラミングを学びます」という日本語の文は「私/は/プログラミング/を/学び/ます」という風になります。\n",
    "\n",
    "\n",
    "これには **MeCab** や **Janome** のような形態素解析ツールを用います。Pythonから利用することも可能です。MeCabをウェブ上で簡単に利用できるWeb[茶まめ](http://chamame.ninjal.ac.jp/)というサービスも国立国語研究所が提供しています。\n",
    "\n",
    "\n",
    "自然言語では新しい言葉も日々生まれますので、それにどれだけ対応できるかも大切です。MeCab用の毎週更新される辞書として **mecab-ipadic-NEologd** がオープンソースで存在しています。\n",
    "\n",
    "\n",
    "[mecab-ipadic-neologd/README.ja.md at master · neologd/mecab-ipadic-neologd](https://github.com/neologd/mecab-ipadic-neologd/blob/master/README.ja.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-essex",
   "metadata": {},
   "source": [
    "**<u>n-gram<u/>**\n",
    "    \n",
    "上のBoWの例では1つの単語（トークン）毎の登場回数を数えましたが、これでは語順はまったく考慮されていません。\n",
    "\n",
    "\n",
    "考慮するために、隣あう単語同士をまとめて扱う **n-gram** という考え方を適用することがあります。2つの単語をまとめる場合は **2-gram (bigram)** と呼び、次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "exterior-imperial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a good</th>\n",
       "      <th>bad very</th>\n",
       "      <th>film is</th>\n",
       "      <th>is a</th>\n",
       "      <th>is very</th>\n",
       "      <th>movie is</th>\n",
       "      <th>this film</th>\n",
       "      <th>this movie</th>\n",
       "      <th>very bad</th>\n",
       "      <th>very good</th>\n",
       "      <th>very very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a good  bad very  film is  is a  is very  movie is  this film  this movie  \\\n",
       "0       0         0        0     0        1         1          0           1   \n",
       "1       1         0        1     1        0         0          1           0   \n",
       "2       0         1        0     0        0         0          0           0   \n",
       "\n",
       "   very bad  very good  very very  \n",
       "0         0          1          0  \n",
       "1         0          0          0  \n",
       "2         2          0          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ngram_rangeで利用するn-gramの範囲を指定する\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-willow",
   "metadata": {},
   "source": [
    "2-gramにより「very good」と「very bad」が区別して数えられています。\n",
    "\n",
    "\n",
    "単語をまとめない場合は **1-gram (unigram)** と呼びます。3つまとめる3-gram(trigram)など任意の数を考えることができます。1-gramと2-gramを組み合わせてBoWを行うといったこともあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-responsibility",
   "metadata": {},
   "source": [
    "## 【問題1】BoWのスクラッチ実装\n",
    "以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください。\n",
    "\n",
    "This movie is SOOOO funny!!!\n",
    "\n",
    "What a movie! I never\n",
    "\n",
    "best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "incoming-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # 正規表現操作\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reflected-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bow(texts, gram=1):\n",
    "    \n",
    "    # 辞書を作成\n",
    "    vocabulary = []\n",
    "    if gram == 1: # 1-gramの時の処理\n",
    "        \n",
    "        for i in range(len(texts)):\n",
    "            \n",
    "            # 単語の分割\n",
    "            tokens = re.split(' ', texts[i])\n",
    "            # 辞書に追加\n",
    "            for token in tokens:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary.append(token)       \n",
    "                    \n",
    "    else: # 2-gramの時の処理\n",
    "        for i in range(len(texts)):\n",
    "            \n",
    "            # 単語の分割\n",
    "            tokens = re.split(' ', texts[i])\n",
    "            # 辞書に追加\n",
    "            for j in range(len(tokens) - 1):\n",
    "                vocabulary.append(tokens[j] + ' ' +tokens[j + 1])\n",
    "        vocabulary = list(set(vocabulary)) # 重複をなくす\n",
    "    \n",
    "    # bowを入れる箱を用意\n",
    "    bow = np.zeros([len(texts), len(vocabulary)])\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        for j in range(len(vocabulary)):\n",
    "            bow[i][j] = texts[i].count(vocabulary[j])\n",
    "            \n",
    "    return vocabulary, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "structural-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力文のlist\n",
    "texts = [\n",
    "    'This movie is SOOOO funny!!!',\n",
    "    'What a movie! I never',\n",
    "    'best movie ever!!!!! this movie',\n",
    "]\n",
    "\n",
    "# 特殊文字（!）の除去、大文字の小文字化\n",
    "for i in range(len(texts)):\n",
    "    texts[i] = texts[i].lower()\n",
    "    texts[i] = texts[i].replace('!', '').replace('soooo', 'so') # 複数の文字列を置換"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-minimum",
   "metadata": {},
   "source": [
    "(参考記事)[Pythonで文字列を置換（replace, translate, re.sub, re.subn）](https://note.nkmk.me/python-str-replace-translate-re-sub/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "closed-albany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this movie is so funny',\n",
       " 'what a movie i never',\n",
       " 'best movie ever this movie']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "broken-degree",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>so</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this  movie  is  so  funny  what  a  i  never  best  ever\n",
       "0     1      1   2   1      1     0  0  3      0     0     0\n",
       "1     0      1   0   0      0     1  2  2      1     0     1\n",
       "2     1      2   1   0      0     0  0  3      0     1     1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-gram\n",
    "\n",
    "vocabulary01, bow01 = calc_bow(texts, gram=1) # vocabulary01:list, bow01:ndarray\n",
    "pd.DataFrame(bow01, columns=vocabulary01).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "coastal-chess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>what a</th>\n",
       "      <th>movie ever</th>\n",
       "      <th>a movie</th>\n",
       "      <th>this movie</th>\n",
       "      <th>movie is</th>\n",
       "      <th>movie i</th>\n",
       "      <th>is so</th>\n",
       "      <th>best movie</th>\n",
       "      <th>ever this</th>\n",
       "      <th>i never</th>\n",
       "      <th>so funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   what a  movie ever  a movie  this movie  movie is  movie i  is so  \\\n",
       "0       0           0        0           1         1        1      1   \n",
       "1       1           0        1           0         0        1      0   \n",
       "2       0           1        0           1         0        0      0   \n",
       "\n",
       "   best movie  ever this  i never  so funny  \n",
       "0           0          0        0         1  \n",
       "1           0          0        1         0  \n",
       "2           1          1        0         0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-gram\n",
    "\n",
    "vocabulary02, bow02 = calc_bow(texts, gram=2) # vocabulary02:list, bow02:ndarray\n",
    "pd.DataFrame(bow02, columns=vocabulary02).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-night",
   "metadata": {},
   "source": [
    "## 6.TF-IDF\n",
    "\n",
    "BoWの発展的手法として **TF-IDF** もよく使われます。これは **Term Frequency (TF)** と **Inverse Document Frequency (IDF)** という2つの指標の組み合わせです。\n",
    "\n",
    "**<u>IDF</u>**\n",
    "\n",
    "IDFはそのトークンがデータセット内で珍しいほど値が大きくなる指標です。\n",
    "\n",
    "\n",
    "サンプル数 N をIMDB映画レビューデータセットの訓練データに合わせ25000として、トークンが出現するサンプル数 df(t) を変化させたグラフを確認してみると、次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hearing-participation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n_samples = 25000\n",
    "idf = np.log(n_samples/np.arange(1,n_samples))\n",
    "plt.title(\"IDF\")\n",
    "plt.xlabel(\"df(t)\")\n",
    "plt.ylabel(\"IDF\")\n",
    "plt.plot(idf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-armor",
   "metadata": {},
   "source": [
    "TF-IDFではこの数を出現回数に掛け合わせるので、珍しいトークンの登場に重み付けを行なっていることになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-refund",
   "metadata": {},
   "source": [
    "**<u>ストップワード</u>**\n",
    "\n",
    "あまりにも頻繁に登場するトークンは、値を小さくするだけでなく、取り除くという前処理を加えることもあります。取り除くもののことを **ストップワード** と呼びます。既存のストップワード一覧を利用したり、しきい値によって求めたりします。\n",
    "\n",
    "\n",
    "scikit-learnのCountVectorizerでは引数stop_wordsにリストで指定することで処理を行なってくれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "running-genealogy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  movie  this  very\n",
       "0  0    0     0     1      1     1     1\n",
       "1  1    0     1     1      0     1     0\n",
       "2  0    2     0     0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-audit",
   "metadata": {},
   "source": [
    "代表的な既存のストップワード一覧としては、**NLTK** という自然言語処理のライブラリのものがあげられます。あるデータセットにおいては特別重要な意味を持つ単語が一覧に含まれている可能性もあるため、使用する際は中身を確認することが望ましいです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mature-player",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wuchuan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# はじめて使う場合はストップワードをダウンロード\n",
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-projector",
   "metadata": {},
   "source": [
    "逆に、登場回数が特に少ないトークンも取り除くことが多いです。すべてのトークンを用いるとベクトルの次元数が著しく大きくなってしまい計算コストが高まるためです。\n",
    "\n",
    "\n",
    "scikit-learnのCountVectorizerでは引数max_featuresに最大の語彙数を指定することで処理を行なってくれます。以下の例では出現数が多い順に5個でベクトル化しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "devoted-zambia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  good  is  this  very\n",
       "0    0     1   1     1     1\n",
       "1    0     1   1     1     0\n",
       "2    2     0   0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-blues",
   "metadata": {},
   "source": [
    "## 【問題2】TF-IDFの計算\n",
    "IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。\n",
    "\n",
    "\n",
    "TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。\n",
    "\n",
    "\n",
    "[sklearn.feature_extraction.text.TfidfVectorizer — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "[sklearn.feature_extraction.text.TfidfTransformer — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n",
    "\n",
    "\n",
    "なお、scikit-learnでは標準的な式とは異なる式が採用されています。\n",
    "\n",
    "\n",
    "また、デフォルトではnorm=\"l2\"の引数が設定されており、各サンプルにL2正規化が行われます。norm=Noneとすることで正規化は行われなくなります。\n",
    "\n",
    "詳細は以下のドキュメントを確認してください。\n",
    "\n",
    "\n",
    "[5.2.3.4. Tf–idf term weighting — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "relative-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "solved-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTKのストップワードを利用,最大の語彙数は5000,1-gram(ユニグラム)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "vectorizer.fit(x_train)\n",
    "tfidf = vectorizer.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "soviet-facing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "widespread-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4995)\t0.13693218984506625\n",
      "  (0, 4992)\t0.16450474907891674\n",
      "  (0, 4964)\t0.05425587149266504\n",
      "  (0, 4953)\t0.08074683081159045\n",
      "  (0, 4746)\t0.308028981096096\n",
      "  (0, 4639)\t0.20047008228549903\n",
      "  (0, 4515)\t0.05242790153478025\n",
      "  (0, 4474)\t0.12903114072897479\n",
      "  (0, 4444)\t0.10041129167597974\n",
      "  (0, 4443)\t0.26258748459210246\n",
      "  (0, 4316)\t0.28032537429141946\n",
      "  (0, 4045)\t0.14959008783968586\n",
      "  (0, 3884)\t0.05436294092381457\n",
      "  (0, 3635)\t0.14016268714570973\n",
      "  (0, 3436)\t0.14650744967796336\n",
      "  (0, 3236)\t0.09997509327067182\n",
      "  (0, 3136)\t0.11230006872388465\n",
      "  (0, 2967)\t0.08158447663501085\n",
      "  (0, 2942)\t0.03808395521159995\n",
      "  (0, 2932)\t0.16717269022616268\n",
      "  (0, 2907)\t0.09052589699629517\n",
      "  (0, 2821)\t0.09945392614500648\n",
      "  (0, 2609)\t0.130786432007285\n",
      "  (0, 2554)\t0.11722257815761256\n",
      "  (0, 2199)\t0.14375695703613686\n",
      "  :\t:\n",
      "  (24999, 903)\t0.05134792680613674\n",
      "  (24999, 902)\t0.10018930373060742\n",
      "  (24999, 814)\t0.0817152568819128\n",
      "  (24999, 765)\t0.03934671110080626\n",
      "  (24999, 721)\t0.07539525933092615\n",
      "  (24999, 715)\t0.08541427201023195\n",
      "  (24999, 713)\t0.058562396956036\n",
      "  (24999, 709)\t0.0953403053218371\n",
      "  (24999, 690)\t0.06477737895096194\n",
      "  (24999, 636)\t0.500589076316356\n",
      "  (24999, 603)\t0.06827665652722346\n",
      "  (24999, 592)\t0.06880023337454069\n",
      "  (24999, 562)\t0.045688488987360165\n",
      "  (24999, 480)\t0.04744171588824864\n",
      "  (24999, 378)\t0.057361284810144936\n",
      "  (24999, 356)\t0.05408216396371426\n",
      "  (24999, 343)\t0.09172850719752176\n",
      "  (24999, 326)\t0.07731079927955417\n",
      "  (24999, 285)\t0.06667267378622241\n",
      "  (24999, 274)\t0.10634148122847231\n",
      "  (24999, 266)\t0.050234486085227834\n",
      "  (24999, 207)\t0.10518196056440303\n",
      "  (24999, 198)\t0.0802090671695495\n",
      "  (24999, 114)\t0.043681022437253296\n",
      "  (24999, 81)\t0.05801418974130403\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fiscal-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTKのストップワードを利用,最大の語彙数は5000,2-gram(バイグラム)\n",
    "\n",
    "vectorizer02 = TfidfVectorizer(stop_words=stop_words, max_features=5000, ngram_range=(2,2))\n",
    "vectorizer02.fit(x_train)\n",
    "tfidf02 = vectorizer02.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "advanced-venice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "happy-philippines",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4997)\t0.31653922339677404\n",
      "  (0, 4989)\t0.2779033727913634\n",
      "  (0, 4608)\t0.26971915725466644\n",
      "  (0, 4387)\t0.2917543309605415\n",
      "  (0, 3936)\t0.2404561601582769\n",
      "  (0, 3472)\t0.28908290834664513\n",
      "  (0, 2792)\t0.2842264967855029\n",
      "  (0, 1489)\t0.27600167531027925\n",
      "  (0, 1416)\t0.2624972017390703\n",
      "  (0, 1358)\t0.29770717276538994\n",
      "  (0, 1355)\t0.24585376050536575\n",
      "  (0, 1237)\t0.26716271611356407\n",
      "  (0, 522)\t0.12147801672247782\n",
      "  (0, 418)\t0.2494847977201336\n",
      "  (1, 4844)\t0.27189140896566394\n",
      "  (1, 4828)\t0.2184535401771622\n",
      "  (1, 4477)\t0.21450815953193705\n",
      "  (1, 3732)\t0.27189140896566394\n",
      "  (1, 3270)\t0.24045272550203134\n",
      "  (1, 3080)\t0.2371858664845659\n",
      "  (1, 3064)\t0.16410798627320972\n",
      "  (1, 2960)\t0.1795463198210764\n",
      "  (1, 2895)\t0.1883924218177699\n",
      "  (1, 2687)\t0.18961676835650954\n",
      "  (1, 2598)\t0.26989585745590045\n",
      "  :\t:\n",
      "  (24998, 522)\t0.15453030367840245\n",
      "  (24999, 4867)\t0.17625756109686727\n",
      "  (24999, 3905)\t0.14997526782288825\n",
      "  (24999, 3693)\t0.2302370792703867\n",
      "  (24999, 3513)\t0.23955174851445424\n",
      "  (24999, 3447)\t0.17154503975854504\n",
      "  (24999, 3089)\t0.22312305662766305\n",
      "  (24999, 2930)\t0.18869070042516944\n",
      "  (24999, 2859)\t0.21979078690395756\n",
      "  (24999, 2757)\t0.19153276272764794\n",
      "  (24999, 2275)\t0.19439518467508243\n",
      "  (24999, 2256)\t0.3713607973777533\n",
      "  (24999, 2120)\t0.1554857623633538\n",
      "  (24999, 2114)\t0.20212546965938114\n",
      "  (24999, 2112)\t0.22608082997453305\n",
      "  (24999, 1672)\t0.16669251595760937\n",
      "  (24999, 1620)\t0.2185541615461723\n",
      "  (24999, 1534)\t0.18350862688499356\n",
      "  (24999, 1280)\t0.15858526949642032\n",
      "  (24999, 831)\t0.2050832430062511\n",
      "  (24999, 522)\t0.04645097523479676\n",
      "  (24999, 506)\t0.20587012139239022\n",
      "  (24999, 272)\t0.21795427828831052\n",
      "  (24999, 239)\t0.18077766937335138\n",
      "  (24999, 183)\t0.20793167275270252\n"
     ]
    }
   ],
   "source": [
    "print(tfidf02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-durham",
   "metadata": {},
   "source": [
    "## 【問題3】TF-IDFを用いた学習\n",
    "\n",
    "問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。モデルは2値分類が行える任意のものを利用してください。\n",
    "\n",
    "\n",
    "ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dying-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-vault",
   "metadata": {},
   "source": [
    "**NLTKのストップワード,最大語彙数5000,1-gram(ユニグラム)で学習・推定**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "joined-housing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (20000, 5000)\n",
      "X_val.shape:  (5000, 5000)\n",
      "y_train.shape:  (20000,)\n",
      "y_val.shape:  (5000,)\n"
     ]
    }
   ],
   "source": [
    "# データ分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(tfidf, y_train, test_size=0.2)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('X_val.shape: ', X_val.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "medieval-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMによる学習・推定\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred01 = classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "different-outline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.889"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解率\n",
    "\n",
    "accuracy_score(y_pred01, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-evolution",
   "metadata": {},
   "source": [
    "**NLTKのストップワード,最大語彙数100,1-gram(ユニグラム)で学習・推定**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "editorial-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ再読込み\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "x_test, y_test = test_review.data, test_review.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "afraid-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_100 = TfidfVectorizer(stop_words=stop_words, max_features=100, ngram_range=(1,1))\n",
    "tfidf_100 = vectorizer_100.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "convenient-substance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (20000, 100)\n",
      "X_val.shape:  (5000, 100)\n",
      "y_train.shape:  (20000,)\n",
      "y_val.shape:  (5000,)\n"
     ]
    }
   ],
   "source": [
    "# データ分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(tfidf_100, y_train, test_size=0.2)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('X_val.shape: ', X_val.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "variable-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMによる学習・推定\n",
    "\n",
    "classifier02 = SVC()\n",
    "classifier02.fit(X_train, y_train)\n",
    "y_pred02 = classifier02.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "neural-label",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解率\n",
    "\n",
    "accuracy_score(y_pred02, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-picture",
   "metadata": {},
   "source": [
    "**NLTKのストップワード,最大語彙数100,2-gram(バイグラム)で学習・推定**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "existing-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ再読込み\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "x_test, y_test = test_review.data, test_review.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "unable-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_100_2 = TfidfVectorizer(stop_words=stop_words, max_features=100, ngram_range=(2,2))\n",
    "tfidf_100_2 = vectorizer_100_2.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "congressional-retention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (20000, 100)\n",
      "X_val.shape:  (5000, 100)\n",
      "y_train.shape:  (20000,)\n",
      "y_val.shape:  (5000,)\n"
     ]
    }
   ],
   "source": [
    "# データ分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(tfidf_100_2, y_train, test_size=0.2)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('X_val.shape: ', X_val.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "postal-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMによる学習・推定\n",
    "\n",
    "classifier03 = SVC()\n",
    "classifier03.fit(X_train, y_train)\n",
    "y_pred03 = classifier03.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "accompanied-arnold",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6476"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解率\n",
    "\n",
    "accuracy_score(y_pred03, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-aquatic",
   "metadata": {},
   "source": [
    "**ストップワード無し,最大語彙数100,2-gram(バイグラム)で学習・推定**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "orange-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ再読込み\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "x_test, y_test = test_review.data, test_review.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "innocent-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_100_3 = TfidfVectorizer(max_features=100, ngram_range=(2,2))\n",
    "tfidf_100_3 = vectorizer_100_3.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "protecting-whole",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (20000, 100)\n",
      "X_val.shape:  (5000, 100)\n",
      "y_train.shape:  (20000,)\n",
      "y_val.shape:  (5000,)\n"
     ]
    }
   ],
   "source": [
    "# データ分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(tfidf_100_3, y_train, test_size=0.2)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('X_val.shape: ', X_val.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "subject-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMによる学習・推定\n",
    "\n",
    "classifier04 = SVC()\n",
    "classifier04.fit(X_train, y_train)\n",
    "y_pred04 = classifier04.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "worldwide-deadline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6748"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解率\n",
    "\n",
    "accuracy_score(y_pred04, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-running",
   "metadata": {},
   "source": [
    "## 【問題4】TF-IDFのスクラッチ実装\n",
    "\n",
    "以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。\n",
    "\n",
    "This movie is SOOOO funny!!!\n",
    "\n",
    "What a movie! I never\n",
    "\n",
    "best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-arthritis",
   "metadata": {},
   "source": [
    "**標準的なTF-IDFの式で実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "convertible-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tfidf(texts, gram=1):\n",
    "    \n",
    "    # 辞書を作成\n",
    "    vocabulary = []\n",
    "    if gram == 1: # 1-gramの時の処理\n",
    "        \n",
    "        for i in range(len(texts)):\n",
    "            \n",
    "            # 単語の分割\n",
    "            tokens = re.split(' ', texts[i])\n",
    "            # 辞書に追加\n",
    "            for token in tokens:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary.append(token)       \n",
    "                    \n",
    "    else: # 2-gramの時の処理\n",
    "        for i in range(len(texts)):\n",
    "            \n",
    "            # 単語の分割\n",
    "            tokens = re.split(' ', texts[i])\n",
    "            # 辞書に追加\n",
    "            for j in range(len(tokens) - 1):\n",
    "                vocabulary.append(tokens[j] + ' ' +tokens[j + 1])\n",
    "        vocabulary = list(set(vocabulary)) # 重複をなくす\n",
    "    \n",
    "    # tfidfを入れる箱を用意\n",
    "    tfidf = np.zeros([len(texts), len(vocabulary)])\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        for j in range(len(vocabulary)):\n",
    "            tf = texts[i].count(vocabulary[j]) / len(re.split(' ', texts[i]))\n",
    "            df = 0\n",
    "            for text in texts:\n",
    "                df += 1 if vocabulary[j] in text else 0\n",
    "            idf = np.log(len(texts) / (df + 1)) # IDfの分母が0になることを防ぐため1を足す           \n",
    "            tfidf[i][j] = tf*idf\n",
    "            \n",
    "    return vocabulary, tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-soldier",
   "metadata": {},
   "source": [
    "(参考記事)[自然言語処理の基礎であるTF-IDFの計算方法とPythonによる実装方法を解説](https://note.com/shimakaze_soft/n/n6e92d1a4851b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "extreme-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力文のlist\n",
    "texts = [\n",
    "    'This movie is SOOOO funny!!!',\n",
    "    'What a movie! I never',\n",
    "    'best movie ever!!!!! this movie',\n",
    "]\n",
    "\n",
    "# 特殊文字（!）の除去、大文字の小文字化\n",
    "for i in range(len(texts)):\n",
    "    texts[i] = texts[i].lower()\n",
    "    texts[i] = texts[i].replace('!', '').replace('soooo', 'so') # 複数の文字列を置換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "floating-distribution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this movie is so funny',\n",
       " 'what a movie i never',\n",
       " 'best movie ever this movie']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "roman-latex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>so</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.057536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.172609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.057536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.162186</td>\n",
       "      <td>-0.115073</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.115073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.172609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this     movie   is        so     funny      what         a         i  \\\n",
       "0   0.0 -0.057536  0.0  0.081093  0.081093  0.000000  0.000000 -0.172609   \n",
       "1   0.0 -0.057536  0.0  0.000000  0.000000  0.081093  0.162186 -0.115073   \n",
       "2   0.0 -0.115073  0.0  0.000000  0.000000  0.000000  0.000000 -0.172609   \n",
       "\n",
       "      never      best  ever  \n",
       "0  0.000000  0.000000   0.0  \n",
       "1  0.081093  0.000000   0.0  \n",
       "2  0.000000  0.081093   0.0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-gram\n",
    "\n",
    "vocabulary01, tfidf01 = calc_tfidf(texts, gram=1) # vocabulary01:list, bow01:ndarray\n",
    "pd.DataFrame(tfidf01, columns=vocabulary01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "joint-surface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i never</th>\n",
       "      <th>ever this</th>\n",
       "      <th>is so</th>\n",
       "      <th>what a</th>\n",
       "      <th>movie ever</th>\n",
       "      <th>best movie</th>\n",
       "      <th>a movie</th>\n",
       "      <th>this movie</th>\n",
       "      <th>movie i</th>\n",
       "      <th>movie is</th>\n",
       "      <th>so funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.081093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    i never  ever this     is so    what a  movie ever  best movie   a movie  \\\n",
       "0  0.000000   0.000000  0.081093  0.000000    0.000000    0.000000  0.000000   \n",
       "1  0.081093   0.000000  0.000000  0.081093    0.000000    0.000000  0.081093   \n",
       "2  0.000000   0.081093  0.000000  0.000000    0.081093    0.081093  0.000000   \n",
       "\n",
       "   this movie  movie i  movie is  so funny  \n",
       "0         0.0      0.0  0.081093  0.081093  \n",
       "1         0.0      0.0  0.000000  0.000000  \n",
       "2         0.0      0.0  0.000000  0.000000  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-gram\n",
    "\n",
    "vocabulary02, tfidf02 = calc_tfidf(texts, gram=2) # vocabulary02:list, bow02:ndarray\n",
    "pd.DataFrame(tfidf02, columns=vocabulary02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-sucking",
   "metadata": {},
   "source": [
    "**scikit-learnの採用している式で実装**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "written-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tfidf02(texts, gram=1):\n",
    "    \n",
    "    # 辞書を作成\n",
    "    vocabulary = []\n",
    "    if gram == 1: # 1-gramの時の処理\n",
    "        \n",
    "        for i in range(len(texts)):\n",
    "            \n",
    "            # 単語の分割\n",
    "            tokens = re.split(' ', texts[i])\n",
    "            # 辞書に追加\n",
    "            for token in tokens:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary.append(token)       \n",
    "                    \n",
    "    else: # 2-gramの時の処理\n",
    "        for i in range(len(texts)):\n",
    "            \n",
    "            # 単語の分割\n",
    "            tokens = re.split(' ', texts[i])\n",
    "            # 辞書に追加\n",
    "            for j in range(len(tokens) - 1):\n",
    "                vocabulary.append(tokens[j] + ' ' +tokens[j + 1])\n",
    "        vocabulary = list(set(vocabulary)) # 重複をなくす\n",
    "    \n",
    "    # tfidfを入れる箱を用意\n",
    "    tfidf = np.zeros([len(texts), len(vocabulary)])\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        for j in range(len(vocabulary)):\n",
    "            tf = texts[i].count(vocabulary[j])\n",
    "            df = 0\n",
    "            for text in texts:\n",
    "                df += 1 if vocabulary[j] in text else 0\n",
    "            idf = np.log((len(texts) + 1) / (df + 1)) + 1 # IDfの分母が0になることを防ぐため1を足す           \n",
    "            tfidf[i][j] = tf*idf\n",
    "            \n",
    "    return vocabulary, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "russian-valuable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>so</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.575364</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>3.386294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.287682</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.287682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       this  movie        is        so     funny      what         a    i  \\\n",
       "0  1.287682    1.0  2.575364  1.693147  1.693147  0.000000  0.000000  3.0   \n",
       "1  0.000000    1.0  0.000000  0.000000  0.000000  1.693147  3.386294  2.0   \n",
       "2  1.287682    2.0  1.287682  0.000000  0.000000  0.000000  0.000000  3.0   \n",
       "\n",
       "      never      best      ever  \n",
       "0  0.000000  0.000000  0.000000  \n",
       "1  1.693147  0.000000  1.287682  \n",
       "2  0.000000  1.693147  1.287682  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-gram\n",
    "\n",
    "vocabulary03, tfidf03 = calc_tfidf02(texts, gram=1) # vocabulary03:list, bow03:ndarray\n",
    "pd.DataFrame(tfidf03, columns=vocabulary03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "becoming-melbourne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i never</th>\n",
       "      <th>ever this</th>\n",
       "      <th>is so</th>\n",
       "      <th>what a</th>\n",
       "      <th>movie ever</th>\n",
       "      <th>best movie</th>\n",
       "      <th>a movie</th>\n",
       "      <th>this movie</th>\n",
       "      <th>movie i</th>\n",
       "      <th>movie is</th>\n",
       "      <th>so funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    i never  ever this     is so    what a  movie ever  best movie   a movie  \\\n",
       "0  0.000000   0.000000  1.693147  0.000000    0.000000    0.000000  0.000000   \n",
       "1  1.693147   0.000000  0.000000  1.693147    0.000000    0.000000  1.693147   \n",
       "2  0.000000   1.693147  0.000000  0.000000    1.693147    1.693147  0.000000   \n",
       "\n",
       "   this movie   movie i  movie is  so funny  \n",
       "0    1.287682  1.287682  1.693147  1.693147  \n",
       "1    0.000000  1.287682  0.000000  0.000000  \n",
       "2    1.287682  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-gram\n",
    "\n",
    "vocabulary04, tfidf04 = calc_tfidf02(texts, gram=2) # vocabulary04:list, bow04:ndarray\n",
    "pd.DataFrame(tfidf04, columns=vocabulary04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-extreme",
   "metadata": {},
   "source": [
    "## 7.Word2Vec\n",
    "\n",
    "ニューラルネットワークを用いてベクトル化を行う手法が **Word2Vec** です。\n",
    "\n",
    "\n",
    "BoWやTF-IDFはone-hot表現であったため、得られるベクトルの次元は語彙数分になります。そのため、語彙数を増やしにくいという問題があります。一方で、Word2Vecでは単語を任意の次元のベクトルに変換します。これをを **Word Embedding（単語埋め込み）** や **分散表現** と呼びます。変換操作を「ベクトル空間に埋め込む」と言うことが多いです。\n",
    "\n",
    "\n",
    "Word2VecにはCBoWとSkip-gramという2種類の仕組みがあるため順番に見ていきます。\n",
    "\n",
    "\n",
    "**<u>CBoW</u>**\n",
    "\n",
    "**CBoW (Continuous Bag-of-Words)** によるWord2Vecではある単語とある単語の間に来る単語を推定できるように全結合層2層のニューラルネットワークを学習します。\n",
    "\n",
    "\n",
    "単語はコーパスの語彙数次元のone-hot表現を行なっておきます。そのため、入力と出力の次元は語彙数と同じになります。一方で、中間のノード数をWord2Vecにより得たい任意の次元数とします。これにより全結合層の重みは「得たい次元のノード数×語彙数」になります。このネットワークにより学習を行なった後、出力側の重みを取り出すことで、各語彙を表すベクトルを手に入れることができます。\n",
    "\n",
    "\n",
    "間の単語の推定を行なっているため、同じ箇所で代替可能な言葉は似たベクトルになるというメリットもあります。これはBoWやTF-IDFでは得られない情報です。\n",
    "\n",
    "\n",
    "あるテキストは「そのテキストの長さ（単語数）×Word2Vecで得た分散表現の次元数」の配列になりますが、各入力の配列を揃える必要があるモデルに入力するためには、短いテキストは空白を表す単語を加える **パディング** を行なったり、長いテキストは単語を消したりします。テキストを **固定長** にすると呼びます。\n",
    "\n",
    "\n",
    "**<u>ウィンドウサイズ</u>**\n",
    "\n",
    "入力する単語は推定する前後1つずつだけでなく、複数個とする場合もあります。前後いくつを見るかの大きさを **ウィンドウサイズ** と呼びます。\n",
    "\n",
    "\n",
    "**<u>Skip-gram</u>**\n",
    "\n",
    "CBoWとは逆にある単語の前後の単語を推定できるように全結合層2層のニューラルネットワークを学習する方法が **Skip-gram** です。学習を行なった後は入力側の重みを取り出し各語彙を表すベクトルとします。現在一般的に使われているのはCBoWよりもSki-gramです。\n",
    "\n",
    "\n",
    "**<u>利用方法</u>**\n",
    "\n",
    "Pythonでは **Gensim** ライブラリを用いて扱うことができます。\n",
    "\n",
    "\n",
    "[gensim: models.word2vec – Word2vec embeddings](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "\n",
    "BoWの例と同じ文章で学習してみます。CountVectorizerと異なり前処理を自動的に行なってはくれないため、単語（トークン）はリストで分割しておきます。また、大文字は小文字に揃え、記号は取り除きます。\n",
    "\n",
    "\n",
    "デフォルトのパラメータではCBoWで計算されます。また、ウィンドウサイズはwindow=5に設定されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "expensive-trading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['this', 'movie', 'is', 'very', 'good', 'film', 'a', 'bad'])\n",
      "thisのベクトル : \n",
      "[-0.04746073 -0.01092739 -0.02214627  0.03811207 -0.01205775 -0.02174545\n",
      "  0.013019    0.03087643  0.00111915 -0.03002558]\n",
      "movieのベクトル : \n",
      "[-0.02683418  0.03933726 -0.0477848   0.03017118 -0.01759759 -0.01259818\n",
      "  0.03497821  0.01782362 -0.03364229 -0.04192084]\n",
      "isのベクトル : \n",
      "[-0.02619042  0.02700837 -0.03979243  0.0326599   0.03928323 -0.0460429\n",
      "  0.04865377  0.04934594 -0.0183245   0.040607  ]\n",
      "veryのベクトル : \n",
      "[-0.00696992  0.03567163 -0.03662668  0.04351414 -0.0331499  -0.04234025\n",
      " -0.01711093 -0.03247644 -0.03449288 -0.03796913]\n",
      "goodのベクトル : \n",
      "[ 0.02156588 -0.01678412 -0.02703536  0.04101167 -0.02677865 -0.02093557\n",
      " -0.03174552  0.03049138  0.03003268 -0.02224186]\n",
      "filmのベクトル : \n",
      "[ 0.01508982 -0.04338841 -0.04397392  0.00362978  0.00716724  0.03569425\n",
      "  0.01648618 -0.001676   -0.03773458 -0.02078286]\n",
      "aのベクトル : \n",
      "[-0.00442331 -0.00107612  0.04157322  0.04101108 -0.00888063 -0.03570421\n",
      " -0.00016483 -0.01945869  0.01963637 -0.04524053]\n",
      "badのベクトル : \n",
      "[ 0.00840995  0.02230871 -0.02820905 -0.0421291   0.03518712  0.04756502\n",
      " -0.01012729 -0.00578692  0.02747044 -0.00030448]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuchuan/.pyenv/versions/anaconda3-2020.11/envs/p3.7t2/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
    "\n",
    "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "for vocab in model.wv.vocab.keys():\n",
    "  print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-spending",
   "metadata": {},
   "source": [
    "このようにしてベクトルが得られます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-halifax",
   "metadata": {},
   "source": [
    "**<u>単語の距離</u>**\n",
    "\n",
    "ベクトル間で計算を行うことで、ある単語に似たベクトルを持つ単語を見つけることができます。例えばgoodに似たベクトルの単語を3つ探します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aware-mining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0.4481246769428253),\n",
       " ('movie', 0.3299741744995117),\n",
       " ('bad', 0.228041872382164)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"good\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "three-worse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0.4481246769428253),\n",
       " ('movie', 0.3299741744995117),\n",
       " ('bad', 0.228041872382164),\n",
       " ('film', 0.07931263744831085),\n",
       " ('a', -0.15899249911308289),\n",
       " ('very', -0.38713887333869934),\n",
       " ('is', -0.4803425669670105)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lined-flashing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0.4481246769428253),\n",
       " ('movie', 0.3299741744995117),\n",
       " ('bad', 0.228041872382164),\n",
       " ('film', 0.07931263744831085),\n",
       " ('a', -0.15899249911308289),\n",
       " ('very', -0.38713887333869934),\n",
       " ('is', -0.4803425669670105)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-developer",
   "metadata": {
    "tags": []
   },
   "source": [
    "今の例では3文しか学習していませんので効果を発揮しませんが、大きなコーパスで学習することで、並列関係のものが近くに来たりなど面白い結果が得られます。\n",
    "\n",
    "\n",
    "**<u>可視化</u>**\n",
    "\n",
    "2次元に圧縮することで単語ごとの位置関係を可視化することができます。以下はt-SNEを用いた例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "twenty-respect",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuchuan/.pyenv/versions/anaconda3-2020.11/envs/p3.7t2/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "vocabs = model.wv.vocab.keys()\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-hearts",
   "metadata": {},
   "source": [
    "## 8.IMDB映画レビューデータセットの分散表現\n",
    "\n",
    "IMDB映画レビューデータセットの訓練データをコーパスとしてWord2Vecを学習させ分散表現を獲得しましょう。\n",
    "\n",
    "## 【問題5】コーパスの前処理\n",
    "コーパスの前処理として、特殊文字（!など）やURLの除去、大文字の小文字化といったことを行なってください。また、単語（トークン）はリストで分割してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-transaction",
   "metadata": {},
   "source": [
    "**<u>サンプルデータで動作確認</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "expressed-principal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This movie is SOOOO funny!!!',\n",
       " 'What a movie! I never',\n",
       " 'best movie ever!!!!! this movie',\n",
       " \"I don't like the movie.\"]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textｓ = [\n",
    "'This movie is SOOOO funny!!!',\n",
    "'What a movie! I never',\n",
    "'best movie ever!!!!! this movie',\n",
    "'I don\\'t like the movie.'\n",
    "]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "equipped-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 短縮表現を元に戻す準備\n",
    "shortened = {\n",
    "    '\\'m': ' am',\n",
    "    '\\'re': ' are',\n",
    "    'don\\'t': 'do not',\n",
    "    'doesn\\'t': 'does not',\n",
    "    'didn\\'t': 'did not',\n",
    "    'won\\'t': 'will not',\n",
    "    'wanna': 'want to',\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    'hafta': 'have to',\n",
    "    'needa': 'need to',\n",
    "    'outta': 'out of',\n",
    "    'kinda': 'kind of',\n",
    "    'sorta': 'sort of',\n",
    "    'lotta': 'lot of',\n",
    "    'lemme': 'let me',\n",
    "    'gimme': 'give me',\n",
    "    'getcha': 'get you',\n",
    "    'gotcha': 'got you',\n",
    "    'letcha': 'let you',\n",
    "    'betcha': 'bet you',\n",
    "    'shoulda': 'should have',\n",
    "    'coulda': 'could have',\n",
    "    'woulda': 'would have',\n",
    "    'musta': 'must have',\n",
    "    'mighta': 'might have',\n",
    "    'dunno': 'do not know',\n",
    "}\n",
    "\n",
    "shortened_re = re.compile('(?:' + '|'.join(map(lambda x: '\\\\b' + x + '\\\\b', shortened.keys())) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-directory",
   "metadata": {},
   "source": [
    "(参考記事)[自然言語処理の前処理・素性いろいろ](https://yukinoi.hatenablog.com/entry/2018/05/29/120000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "gorgeous-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # サンプルデータの前処理\n",
    "# sentence_list = []\n",
    "# for i in range(len(texts)):\n",
    "#     texts[i] = shortened_re.sub(lambda x: shortened[x.group(0)], texts[i]) # 短縮表現を元に戻す\n",
    "#     texts[i] = texts[i].lower() # 大文字の小文字化\n",
    "#     texts[i] = texts[i].replace('!', '').replace('soooo', 'so')\\\n",
    "#     .replace(r\"[^a-zA-Z0-9]+\", \"\").replace('.', '') # 複数の文字列を置換\n",
    "#     texts[i] = texts[i].rstrip() # 余分な改行やスペースなどを除去\n",
    "#     tokens = re.split('[, /]', texts[i])   \n",
    "#     sentence_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "introductory-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータの前処理\n",
    "text_list = []\n",
    "for i in range(len(texts)):\n",
    "    texts[i] = shortened_re.sub(lambda x: shortened[x.group(0)], texts[i]) # 短縮表現を元に戻す\n",
    "    texts[i] = texts[i].lower() # 大文字の小文字化\n",
    "    texts[i] = texts[i].replace('!', '').replace('soooo', 'so')\\\n",
    "    .replace(r\"[^a-zA-Z0-9]+\", \"\").replace('.', '') # 複数の文字列を置換\n",
    "    texts[i] = texts[i].rstrip() # 余分な改行やスペースなどを除去\n",
    "    tokens = re.split('[, /]', texts[i])   \n",
    "    tokens = list(set(tokens)) # 重複をなくす\n",
    "   \n",
    "    # 単語（トークン）をリストで分割\n",
    "    for token in tokens:\n",
    "        text_list.append(token.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cheap-virtue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this'],\n",
       " ['so'],\n",
       " ['funny'],\n",
       " ['is'],\n",
       " ['movie'],\n",
       " ['a'],\n",
       " ['never'],\n",
       " ['i'],\n",
       " ['movie'],\n",
       " ['what'],\n",
       " ['this'],\n",
       " ['ever'],\n",
       " ['movie'],\n",
       " ['best'],\n",
       " ['not'],\n",
       " ['do'],\n",
       " ['like'],\n",
       " ['the'],\n",
       " ['i'],\n",
       " ['movie']]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "specified-pakistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['this', 'so', 'funny', 'is', 'movie', 'a', 'never', 'i', 'what', 'ever', 'best', 'not', 'do', 'like', 'the'])\n",
      "thisのベクトル : \n",
      "[-0.04746073 -0.01092739 -0.02214627  0.03811207 -0.01205775 -0.02174545\n",
      "  0.013019    0.03087643  0.00111915 -0.03002558]\n",
      "soのベクトル : \n",
      "[ 0.01668941  0.03692876 -0.04896627  0.04703923  0.0240517   0.00656861\n",
      "  0.01695647  0.01552369 -0.00166708  0.04215109]\n",
      "funnyのベクトル : \n",
      "[-0.02344675  0.0048656   0.01793307 -0.04315045 -0.01967878 -0.0286889\n",
      " -0.01397028 -0.02253332 -0.0142773   0.03384078]\n",
      "isのベクトル : \n",
      "[-0.02619026  0.0270082  -0.03979218  0.03265969  0.03928298 -0.04604261\n",
      "  0.04865346  0.04934563 -0.01832438  0.04060674]\n",
      "movieのベクトル : \n",
      "[-0.02683418  0.03933726 -0.0477848   0.03017118 -0.01759759 -0.01259818\n",
      "  0.03497821  0.01782362 -0.03364229 -0.04192084]\n",
      "aのベクトル : \n",
      "[-0.00442331 -0.00107612  0.04157322  0.04101108 -0.00888063 -0.03570421\n",
      " -0.00016483 -0.01945869  0.01963637 -0.04524053]\n",
      "neverのベクトル : \n",
      "[ 0.04145455  0.04145847 -0.02399435  0.00848846  0.01394163  0.01772032\n",
      "  0.00024095 -0.00519116 -0.01540093  0.01598135]\n",
      "iのベクトル : \n",
      "[-0.01001779 -0.00616553  0.03956485 -0.0389199   0.02593871  0.02597461\n",
      " -0.02357634 -0.00679887 -0.02893548 -0.03691804]\n",
      "whatのベクトル : \n",
      "[-0.02601345  0.0224518   0.037759   -0.02971794  0.04810886 -0.0218183\n",
      " -0.04481572  0.01711915 -0.0192398  -0.04583773]\n",
      "everのベクトル : \n",
      "[ 0.03472843 -0.04654685 -0.00061356  0.03988094  0.04870518  0.02249823\n",
      "  0.01479982  0.03152203 -0.01899965 -0.04850025]\n",
      "bestのベクトル : \n",
      "[ 0.03376051 -0.0494532   0.0272796   0.01373148 -0.03832761  0.04708677\n",
      "  0.00167954 -0.01096185 -0.01442621  0.04460147]\n",
      "notのベクトル : \n",
      "[ 0.01976683 -0.04696994  0.04581403 -0.04240819  0.01677138  0.00795708\n",
      " -0.03578747 -0.02378275 -0.02509569 -0.03199894]\n",
      "doのベクトル : \n",
      "[ 0.00462024  0.00347825 -0.02894647 -0.03235656 -0.01829538  0.01630688\n",
      " -0.03386026  0.02915226  0.02962053  0.04101862]\n",
      "likeのベクトル : \n",
      "[-0.01726446  0.0297963   0.03964626 -0.04400888 -0.04369735 -0.01866724\n",
      "  0.03929735 -0.03298878  0.04690167 -0.02207341]\n",
      "theのベクトル : \n",
      "[ 0.04251569 -0.0181925  -0.02188727  0.01457291  0.02231491  0.00048661\n",
      " -0.00846042 -0.00629035  0.00074524  0.02128999]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuchuan/.pyenv/versions/anaconda3-2020.11/envs/p3.7t2/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(text_list) # 準備\n",
    "model.train(text_list, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
    "\n",
    "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "for vocab in model.wv.vocab.keys():\n",
    "  print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "gentle-notion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuchuan/.pyenv/versions/anaconda3-2020.11/envs/p3.7t2/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可視化\n",
    "\n",
    "vocabs = model.wv.vocab.keys()\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "compressed-spring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "blank-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB映画レビューデータセットの前処理\n",
    "sentences = []\n",
    "texts = x_train\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    texts[i] = shortened_re.sub(lambda x: shortened[x.group(0)], texts[i]) # 短縮表現を元に戻す\n",
    "    texts[i] = texts[i].lower() # 大文字の小文字化\n",
    "    texts[i] = texts[i].replace('!', '').replace('soooo', 'so').replace('\"', '').replace('-', '')\\\n",
    "    .replace(r\"[^a-zA-Z0-9]+\", \"\").replace('.', '').replace(\"<br />\", \"\") # 複数の文字列を置換\n",
    "    texts[i] = texts[i].rstrip() # 余分な改行やスペースなどを除去\n",
    "    tokens = re.split('[, /]', texts[i])   \n",
    "    tokens = list(set(tokens)) # 重複をなくす\n",
    " \n",
    "    # 単語（トークン）をリストで分割\n",
    "    for token in tokens:\n",
    "        sentences.append(token.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "religious-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IMDB映画レビューデータセットの前処理\n",
    "# sentences = []\n",
    "# texts = x_train\n",
    "\n",
    "# for i in range(len(texts)):\n",
    "#     texts[i] = shortened_re.sub(lambda x: shortened[x.group(0)], texts[i]) # 短縮表現を元に戻す\n",
    "#     texts[i] = texts[i].lower() # 大文字の小文字化\n",
    "#     texts[i] = texts[i].replace('!', '').replace('soooo', 'so').replace(\"'\", '').replace('-', '')\\\n",
    "#     .replace(r\"[^a-zA-Z0-9]+\", \"\").replace('.', '').replace(\"<br />\", \"\") # 複数の文字列を置換\n",
    "#     texts[i] = texts[i].rstrip() # 余分な改行やスペースなどを除去\n",
    "#     tokens = re.split('[, /]', texts[i])\n",
    "#     tokens = list(set(tokens)) # 重複をなくす\n",
    "#     sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "northern-radical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[''], ['two'], ['have'], ['not'], ['in'], ['mutual'], ['beyond'], ['murderers'], ['commit'], ['see'], ['coupled'], ['skin'], ['actions'], ['what'], ['under'], ['define'], ['leads'], ['flawed'], ['honest'], ['world'], ['bizarre'], ['why'], ['via'], ['are'], ['must'], ['slaughtering'], ['their'], ['had'], ['think'], ['they'], ['young'], ['that'], ['almost'], ['did'], ['with'], ['own'], ['it'], ['movie'], ['from'], ['time'], ['who'], ['honesty'], ['our'], ['being'], ['zero'], ['do'], ['explaining'], ['decided'], ['civility'], ['you']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bearing-stretch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3564617"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-mapping",
   "metadata": {},
   "source": [
    "## 【問題6】Word2Vecの学習\n",
    "Word2Vecの学習を行なってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "vocal-disorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wuchuan/.pyenv/versions/anaconda3-2020.11/envs/p3.7t2/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16720377, 17755535)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習\n",
    "\n",
    "model = Word2Vec(min_count=1, size=100) # 次元数を100に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-publisher",
   "metadata": {},
   "source": [
    "## 【問題7】（アドバンス課題）ベクトルの可視化\n",
    "得られたベクトルをt-SNEにより可視化してください。また、いくつかの単語を選びwv.most_similarを用いて似ている単語を調べてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化\n",
    "\n",
    "vocabs = model.wv.vocab.keys()\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-wednesday",
   "metadata": {},
   "source": [
    "**可視化するのに時間がかかりすぎたため、途中でストップ！**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-quilt",
   "metadata": {},
   "source": [
    "**似ている単語を調べる**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "excellent-relationship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thinnedplotted', 0.4096003472805023),\n",
       " ('filmaking', 0.3985108733177185),\n",
       " ('straightfromthecomputer', 0.3934032917022705),\n",
       " ('minivideo', 0.38509392738342285),\n",
       " ('olin)', 0.38455963134765625)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"good\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "aging-onion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stanislavski', 0.4348575472831726),\n",
       " ('spatulas', 0.3927799463272095),\n",
       " ('resonation', 0.39109867811203003),\n",
       " ('cyclical', 0.38855689764022827),\n",
       " ('shiri', 0.3873704671859741)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"bad\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "mobile-culture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$5', 0.466086208820343),\n",
       " ('albizu', 0.40759822726249695),\n",
       " ('killersthey', 0.3908851146697998),\n",
       " ('moviedespite', 0.3891200125217438),\n",
       " ('dolittle', 0.3882330656051636)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"funny\", topn=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3.7t2",
   "language": "python",
   "name": "p3.7t2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
