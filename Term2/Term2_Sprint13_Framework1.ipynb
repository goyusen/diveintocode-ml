{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2dZZXeWziVm"
   },
   "source": [
    "# sprint13 ディープラーニング フレームワーク1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGD5K13c0BFE"
   },
   "source": [
    "## 1.このSprintについて\n",
    "\n",
    "**Sprintの目的**\n",
    "- フレームワークのコードを読めるようにする\n",
    "- フレームワークを習得し続けられるようになる\n",
    "- 理論を知っている範囲をフレームワークで動かす\n",
    "\n",
    "**どのように学ぶか**\n",
    "\n",
    "TensorFLowのサンプルコードを元に、これまで扱ってきたデータセットを学習していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CP0uKp6v0ZG-"
   },
   "source": [
    "# 2.コードリーディング\n",
    "\n",
    "TensorFlowによって2値分類を行うサンプルコードを載せました。今回はこれをベースにして進めます。\n",
    "\n",
    "\n",
    "tf.keras や tf.estimator などの高レベルAPIは使用していません。低レベルなところから見ていくことにします。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW0LdVcM0jN4"
   },
   "source": [
    "## 【問題1】スクラッチを振り返る\n",
    "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。\n",
    "\n",
    "\n",
    "（例）\n",
    "\n",
    "\n",
    "- 重みを初期化する必要があった\n",
    "- エポックのループが必要だった\n",
    "\n",
    "それらがフレームワークにおいてはどのように実装されるかを今回覚えていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7yIPlSo0qh-"
   },
   "source": [
    "**回答**\n",
    "\n",
    "- 学習率の設定\n",
    "- バッチサイズの指定\n",
    "- エポック数の設定\n",
    "- 活性化関数の指定\n",
    "- 重みとバイアスの初期値の設定と更新\n",
    "- 最適化手法の選択\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX0Y5GGw1r5s"
   },
   "source": [
    "> データセットの用意\n",
    "\n",
    "以前から使用しているIrisデータセットを使用します。以下のサンプルコードではIris.csvが同じ階層にある想定です。\n",
    "\n",
    "[Iris Species](https://www.kaggle.com/uciml/iris/data)\n",
    "\n",
    "\n",
    "目的変数はSpeciesですが、3種類ある中から以下の2種類のみを取り出して使用します。\n",
    "\n",
    "\n",
    "Iris-versicolor\n",
    "\n",
    "Iris-virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2147,
     "status": "ok",
     "timestamp": 1615879215261,
     "user": {
      "displayName": "wu yuchuan",
      "photoUrl": "",
      "userId": "11451319653338591748"
     },
     "user_tz": -540
    },
    "id": "G6ctVWfxyUvF",
    "outputId": "a4c03996-4c4b-433a-9aeb-07fb78957be8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 856
    },
    "executionInfo": {
     "elapsed": 51255,
     "status": "ok",
     "timestamp": 1615879194290,
     "user": {
      "displayName": "wu yuchuan",
      "photoUrl": "",
      "userId": "11451319653338591748"
     },
     "user_tz": -540
    },
    "id": "-qnYUZgq2VIF",
    "outputId": "5c77b8ba-8f37-408f-bbd6-aaa39fa43bc7"
   },
   "outputs": [],
   "source": [
    "# Google Colab使う場合\n",
    "# !pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSzxYr2M3Msx"
   },
   "source": [
    "## 【問題2】スクラッチとTensorFlowの対応を考える\n",
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\n",
    "\n",
    "\n",
    "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。\n",
    "\n",
    "\n",
    "**《サンプルコード》**\n",
    "\n",
    "\n",
    "＊TensorFlow バージョン 1.5 から 1.14 までで動作を確認済みです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2649,
     "status": "ok",
     "timestamp": 1615891722204,
     "user": {
      "displayName": "wu yuchuan",
      "photoUrl": "",
      "userId": "11451319653338591748"
     },
     "user_tz": -540
    },
    "id": "YHSCyUp-2-cY",
    "outputId": "2210c0dd-939a-42f9-db79-f5b88122c2a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wuchuan/.pyenv/versions/anaconda3-2020.11/envs/p3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/wuchuan/.pyenv/versions/anaconda3-2020.11/envs/p3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/wuchuan/.pyenv/versions/anaconda3-2020.11/envs/p3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/wuchuan/.pyenv/versions/anaconda3-2020.11/envs/p3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 1.2881, val_loss : 11.5094, acc : 0.250\n",
      "Epoch 1, loss : 0.9409, val_loss : 8.2393, acc : 0.312\n",
      "Epoch 2, loss : 0.6356, val_loss : 4.2207, acc : 0.500\n",
      "Epoch 3, loss : 0.4789, val_loss : 3.2871, acc : 0.562\n",
      "Epoch 4, loss : 0.3392, val_loss : 3.1535, acc : 0.500\n",
      "Epoch 5, loss : 0.2711, val_loss : 2.3081, acc : 0.562\n",
      "Epoch 6, loss : 0.2203, val_loss : 1.8365, acc : 0.688\n",
      "Epoch 7, loss : 0.1893, val_loss : 1.5971, acc : 0.750\n",
      "Epoch 8, loss : 0.1618, val_loss : 1.2165, acc : 0.750\n",
      "Epoch 9, loss : 0.1363, val_loss : 0.9150, acc : 0.812\n",
      "Epoch 10, loss : 0.1162, val_loss : 0.7208, acc : 0.812\n",
      "Epoch 11, loss : 0.0979, val_loss : 0.5280, acc : 0.812\n",
      "Epoch 12, loss : 0.0823, val_loss : 0.3792, acc : 0.875\n",
      "Epoch 13, loss : 0.0717, val_loss : 0.3089, acc : 0.875\n",
      "Epoch 14, loss : 0.0633, val_loss : 0.2742, acc : 0.875\n",
      "Epoch 15, loss : 0.0579, val_loss : 0.2441, acc : 0.875\n",
      "Epoch 16, loss : 0.0537, val_loss : 0.2137, acc : 0.875\n",
      "Epoch 17, loss : 0.0500, val_loss : 0.1857, acc : 0.938\n",
      "Epoch 18, loss : 0.0468, val_loss : 0.1657, acc : 0.938\n",
      "Epoch 19, loss : 0.0449, val_loss : 0.1479, acc : 0.938\n",
      "Epoch 20, loss : 0.0428, val_loss : 0.1432, acc : 0.938\n",
      "Epoch 21, loss : 0.0409, val_loss : 0.1431, acc : 0.938\n",
      "Epoch 22, loss : 0.0394, val_loss : 0.1418, acc : 0.938\n",
      "Epoch 23, loss : 0.0379, val_loss : 0.1450, acc : 0.938\n",
      "Epoch 24, loss : 0.0367, val_loss : 0.1496, acc : 0.938\n",
      "Epoch 25, loss : 0.0359, val_loss : 0.1492, acc : 0.938\n",
      "Epoch 26, loss : 0.0351, val_loss : 0.1520, acc : 0.938\n",
      "Epoch 27, loss : 0.0345, val_loss : 0.1525, acc : 0.938\n",
      "Epoch 28, loss : 0.0340, val_loss : 0.1499, acc : 0.938\n",
      "Epoch 29, loss : 0.0334, val_loss : 0.1507, acc : 0.938\n",
      "Epoch 30, loss : 0.0327, val_loss : 0.1542, acc : 0.938\n",
      "Epoch 31, loss : 0.0321, val_loss : 0.1535, acc : 0.938\n",
      "Epoch 32, loss : 0.0315, val_loss : 0.1568, acc : 0.938\n",
      "Epoch 33, loss : 0.0309, val_loss : 0.1578, acc : 0.938\n",
      "Epoch 34, loss : 0.0303, val_loss : 0.1584, acc : 0.938\n",
      "Epoch 35, loss : 0.0297, val_loss : 0.1601, acc : 0.938\n",
      "Epoch 36, loss : 0.0291, val_loss : 0.1607, acc : 0.938\n",
      "Epoch 37, loss : 0.0287, val_loss : 0.1570, acc : 0.938\n",
      "Epoch 38, loss : 0.0282, val_loss : 0.1566, acc : 0.938\n",
      "Epoch 39, loss : 0.0277, val_loss : 0.1548, acc : 0.938\n",
      "Epoch 40, loss : 0.0272, val_loss : 0.1526, acc : 0.938\n",
      "Epoch 41, loss : 0.0268, val_loss : 0.1514, acc : 0.938\n",
      "Epoch 42, loss : 0.0263, val_loss : 0.1492, acc : 0.938\n",
      "Epoch 43, loss : 0.0259, val_loss : 0.1474, acc : 0.938\n",
      "Epoch 44, loss : 0.0254, val_loss : 0.1455, acc : 0.938\n",
      "Epoch 45, loss : 0.0250, val_loss : 0.1434, acc : 0.938\n",
      "Epoch 46, loss : 0.0245, val_loss : 0.1414, acc : 0.938\n",
      "Epoch 47, loss : 0.0241, val_loss : 0.1391, acc : 0.938\n",
      "Epoch 48, loss : 0.0237, val_loss : 0.1370, acc : 0.938\n",
      "Epoch 49, loss : 0.0233, val_loss : 0.1346, acc : 0.938\n",
      "Epoch 50, loss : 0.0229, val_loss : 0.1323, acc : 0.938\n",
      "Epoch 51, loss : 0.0225, val_loss : 0.1299, acc : 0.938\n",
      "Epoch 52, loss : 0.0221, val_loss : 0.1275, acc : 0.938\n",
      "Epoch 53, loss : 0.0217, val_loss : 0.1250, acc : 0.938\n",
      "Epoch 54, loss : 0.0214, val_loss : 0.1225, acc : 0.938\n",
      "Epoch 55, loss : 0.0210, val_loss : 0.1200, acc : 0.938\n",
      "Epoch 56, loss : 0.0207, val_loss : 0.1175, acc : 0.938\n",
      "Epoch 57, loss : 0.0204, val_loss : 0.1151, acc : 0.938\n",
      "Epoch 58, loss : 0.0200, val_loss : 0.1126, acc : 0.938\n",
      "Epoch 59, loss : 0.0197, val_loss : 0.1103, acc : 0.938\n",
      "Epoch 60, loss : 0.0195, val_loss : 0.1079, acc : 0.938\n",
      "Epoch 61, loss : 0.0192, val_loss : 0.1057, acc : 0.938\n",
      "Epoch 62, loss : 0.0189, val_loss : 0.1034, acc : 0.938\n",
      "Epoch 63, loss : 0.0187, val_loss : 0.1014, acc : 0.938\n",
      "Epoch 64, loss : 0.0184, val_loss : 0.0993, acc : 0.938\n",
      "Epoch 65, loss : 0.0182, val_loss : 0.0974, acc : 0.938\n",
      "Epoch 66, loss : 0.0179, val_loss : 0.0955, acc : 0.938\n",
      "Epoch 67, loss : 0.0177, val_loss : 0.0938, acc : 0.938\n",
      "Epoch 68, loss : 0.0175, val_loss : 0.0920, acc : 0.938\n",
      "Epoch 69, loss : 0.0173, val_loss : 0.0905, acc : 0.938\n",
      "Epoch 70, loss : 0.0171, val_loss : 0.0889, acc : 0.938\n",
      "Epoch 71, loss : 0.0169, val_loss : 0.0876, acc : 0.938\n",
      "Epoch 72, loss : 0.0167, val_loss : 0.0860, acc : 0.938\n",
      "Epoch 73, loss : 0.0165, val_loss : 0.0849, acc : 0.938\n",
      "Epoch 74, loss : 0.0164, val_loss : 0.0834, acc : 0.938\n",
      "Epoch 75, loss : 0.0162, val_loss : 0.0826, acc : 0.938\n",
      "Epoch 76, loss : 0.0160, val_loss : 0.0810, acc : 0.938\n",
      "Epoch 77, loss : 0.0159, val_loss : 0.0804, acc : 0.938\n",
      "Epoch 78, loss : 0.0157, val_loss : 0.0789, acc : 0.938\n",
      "Epoch 79, loss : 0.0155, val_loss : 0.0785, acc : 0.938\n",
      "Epoch 80, loss : 0.0154, val_loss : 0.0769, acc : 0.938\n",
      "Epoch 81, loss : 0.0152, val_loss : 0.0767, acc : 0.938\n",
      "Epoch 82, loss : 0.0151, val_loss : 0.0750, acc : 0.938\n",
      "Epoch 83, loss : 0.0149, val_loss : 0.0751, acc : 0.938\n",
      "Epoch 84, loss : 0.0148, val_loss : 0.0732, acc : 0.938\n",
      "Epoch 85, loss : 0.0146, val_loss : 0.0737, acc : 0.938\n",
      "Epoch 86, loss : 0.0145, val_loss : 0.0714, acc : 0.938\n",
      "Epoch 87, loss : 0.0144, val_loss : 0.0725, acc : 0.938\n",
      "Epoch 88, loss : 0.0142, val_loss : 0.0696, acc : 0.938\n",
      "Epoch 89, loss : 0.0141, val_loss : 0.0716, acc : 0.938\n",
      "Epoch 90, loss : 0.0139, val_loss : 0.0676, acc : 0.938\n",
      "Epoch 91, loss : 0.0138, val_loss : 0.0710, acc : 0.938\n",
      "Epoch 92, loss : 0.0137, val_loss : 0.0653, acc : 0.938\n",
      "Epoch 93, loss : 0.0135, val_loss : 0.0710, acc : 0.938\n",
      "Epoch 94, loss : 0.0134, val_loss : 0.0623, acc : 0.938\n",
      "Epoch 95, loss : 0.0132, val_loss : 0.0719, acc : 0.938\n",
      "Epoch 96, loss : 0.0132, val_loss : 0.0581, acc : 0.938\n",
      "Epoch 97, loss : 0.0129, val_loss : 0.0763, acc : 0.938\n",
      "Epoch 98, loss : 0.0130, val_loss : 0.0489, acc : 0.938\n",
      "Epoch 99, loss : 0.0127, val_loss : 0.0814, acc : 0.938\n",
      "test_acc : 0.900\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import backend as K \n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# データセットの読み込み\n",
    "df = pd.read_csv(\"Iris.csv\")\n",
    "# df = pd.read_csv(\"drive/My Drive/Data/Iris.csv\") # Google Colab使う場合\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "# NumPy 配列に変換\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "# ラベルを数値に変換\n",
    "y[y == \"Iris-versicolor\"] = 0\n",
    "y[y == \"Iris-virginica\"] = 1\n",
    "y = y.astype(np.int64)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "     # summaryの設定\n",
    "    tf.summary.scalar('cross_entropy', loss_op)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter('data', graph=sess.graph)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        \n",
    "        #tf.summary.FileWriter('iris_sigmoid', sess.graph)\n",
    "        summary_str = sess.run(summary_op, feed_dict={X: X_val, Y: y_val})\n",
    "        summary_writer.add_summary(summary_str, epoch)\n",
    "        \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    \n",
    "    # tensorboardでスカラを表示させるために必要\n",
    "    summary_writer.flush()\n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQldxECC9290"
   },
   "source": [
    "**回答**\n",
    "\n",
    "- 学習率の設定 -> 63行目learning_rate = 0.001\n",
    "- バッチサイズの指定 -> 64行目batch_size = 10\n",
    "- エポック数の設定 -> 65行目num_epochs = 100\n",
    "- 活性化関数の指定 -> 110行目 init = tf.global_variables_initializer()\n",
    "- 重みとバイアスの初期値の設定と更新 -> 設定:82~91行目weights = {..., | biases = {...\n",
    "                                  更新:104行目 train_op = optimizer.minimize(loss_op)\n",
    "- 最適化手法の選択 -> 103行目 optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\"\"\"以下を実行すると、TensorBoard 1.14.0 at http://****.local:8010/ (Press CTRL+C to quit) のようにlocalhostのurlが表示されるので、\n",
    "そちらを開くと、新しいブラウザのタブが開き、TensorBoardが表示されます\n",
    "\"\"\"\n",
    "\n",
    "!tensorboard --logdir=data --port=8010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"以下を実行すると、TensorBoard 1.14.0 at http://****.local:8010/ (Press CTRL+C to quit) のようにlocalhostのurlが表示されるので、\n",
    "# そちらを開くと、新しいブラウザのタブが開き、TensorBoardが表示されます\n",
    "# \"\"\"\n",
    "\n",
    "# !tensorboard --logdir=data --port=8010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrkNNlP5AQ-0"
   },
   "source": [
    "## 3.他のデータセットへの適用\n",
    "\n",
    "これまで扱ってきた小さなデータセットが他にもいくつかあります。上記サンプルコードを書き換え、これらに対して学習・推定を行うニューラルネットワークを作成してください。\n",
    "\n",
    "\n",
    "- Iris（3種類全ての目的変数を使用）\n",
    "- House Prices\n",
    "\n",
    "どのデータセットも train, val, test の3種類に分けて使用してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVws6sKMAa2G"
   },
   "source": [
    "## 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成\n",
    "Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類全てを分類できるモデルを作成してください。\n",
    "\n",
    "\n",
    "[Iris Species](https://www.kaggle.com/uciml/iris/data)\n",
    "\n",
    "\n",
    "2クラスの分類と3クラス以上の分類の違いを考慮してください。それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。\n",
    "\n",
    "\n",
    "**《ヒント》**\n",
    "\n",
    "\n",
    "以下の2箇所は2クラス分類特有の処理です。\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "\n",
    "\n",
    "メソッドは以下のように公式ドキュメントを確認してください。\n",
    "\n",
    "\n",
    "[tf.nn.sigmoid_cross_entropy_with_logits  |  TensorFlow](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)\n",
    "\n",
    "\n",
    "[tf.math.sign  |  TensorFlow](https://www.tensorflow.org/api_docs/python/tf/math/sign)\n",
    "\n",
    "\n",
    "＊tf.sign と tf.math.sign は同じ関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "df = pd.read_csv(\"Iris.csv\")\n",
    "\n",
    "# データフレームから条件抽出\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "\n",
    "# NumPy 配列に変換\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "# y = y.astype(np.int64)[:, np.newaxis]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ワンホット処理\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
    "y_one_hot.shape                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (96, 4)\n",
      "X_val.shape (24, 4)\n",
      "y_train (96, 3)\n",
      "y_val.shape (24, 3)\n"
     ]
    }
   ],
   "source": [
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_val.shape', X_val.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('y_val.shape', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "# n_classes = 1 # 2値分類用\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    3クラス以上の分類\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(参考記事)[TensorFlowによる精度計算の流れを追う](https://testpy.hatenablog.com/entry/2016/11/27/035033)\n",
    "- tf.argmax(Y, 1)):行ごとに最大となる列を返す\n",
    "- tf.equal():渡された２つのベクトルが一致しているか否かを見る\n",
    "- tf.cast():第1パラメーターを第2パラメーターのデータ・タイプに変換する\n",
    "- tf.reduce_mean():np.mean()と同じで平均を計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-2eadee1427b4>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "# loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits)) # 2値分類用\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "# correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5)) # 2値分類用\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 8.3052, val_loss : 63.1733, acc : 0.417\n",
      "Epoch 1, loss : 4.5197, val_loss : 29.5834, acc : 0.125\n",
      "Epoch 2, loss : 1.5820, val_loss : 11.5705, acc : 0.542\n",
      "Epoch 3, loss : 0.6664, val_loss : 10.0250, acc : 0.667\n",
      "Epoch 4, loss : 0.5632, val_loss : 8.7899, acc : 0.667\n",
      "Epoch 5, loss : 0.4968, val_loss : 8.5493, acc : 0.625\n",
      "Epoch 6, loss : 0.4111, val_loss : 7.2580, acc : 0.625\n",
      "Epoch 7, loss : 0.3413, val_loss : 5.8605, acc : 0.625\n",
      "Epoch 8, loss : 0.2782, val_loss : 4.9622, acc : 0.625\n",
      "Epoch 9, loss : 0.2183, val_loss : 3.6300, acc : 0.625\n",
      "Epoch 10, loss : 0.1937, val_loss : 3.4547, acc : 0.625\n",
      "Epoch 11, loss : 0.1337, val_loss : 2.3338, acc : 0.667\n",
      "Epoch 12, loss : 0.1114, val_loss : 1.7309, acc : 0.750\n",
      "Epoch 13, loss : 0.0928, val_loss : 1.2694, acc : 0.750\n",
      "Epoch 14, loss : 0.0830, val_loss : 1.1476, acc : 0.875\n",
      "Epoch 15, loss : 0.0684, val_loss : 0.9816, acc : 0.917\n",
      "Epoch 16, loss : 0.0608, val_loss : 0.9326, acc : 0.917\n",
      "Epoch 17, loss : 0.0523, val_loss : 0.8961, acc : 0.917\n",
      "Epoch 18, loss : 0.0465, val_loss : 0.8985, acc : 0.917\n",
      "Epoch 19, loss : 0.0417, val_loss : 0.8926, acc : 0.917\n",
      "Epoch 20, loss : 0.0390, val_loss : 0.9216, acc : 0.917\n",
      "Epoch 21, loss : 0.0356, val_loss : 0.9175, acc : 0.917\n",
      "Epoch 22, loss : 0.0336, val_loss : 0.9143, acc : 0.917\n",
      "Epoch 23, loss : 0.0316, val_loss : 0.9384, acc : 0.917\n",
      "Epoch 24, loss : 0.0301, val_loss : 0.9716, acc : 0.917\n",
      "Epoch 25, loss : 0.0281, val_loss : 0.9603, acc : 0.917\n",
      "Epoch 26, loss : 0.0268, val_loss : 0.9492, acc : 0.917\n",
      "Epoch 27, loss : 0.0252, val_loss : 0.9282, acc : 0.917\n",
      "Epoch 28, loss : 0.0239, val_loss : 0.9254, acc : 0.917\n",
      "Epoch 29, loss : 0.0222, val_loss : 0.9253, acc : 0.917\n",
      "Epoch 30, loss : 0.0206, val_loss : 0.9259, acc : 0.917\n",
      "Epoch 31, loss : 0.0189, val_loss : 0.9221, acc : 0.917\n",
      "Epoch 32, loss : 0.0176, val_loss : 0.9265, acc : 0.917\n",
      "Epoch 33, loss : 0.0159, val_loss : 0.9032, acc : 0.917\n",
      "Epoch 34, loss : 0.0149, val_loss : 0.9163, acc : 0.917\n",
      "Epoch 35, loss : 0.0138, val_loss : 0.9232, acc : 0.917\n",
      "Epoch 36, loss : 0.0127, val_loss : 0.9307, acc : 0.917\n",
      "Epoch 37, loss : 0.0118, val_loss : 0.9384, acc : 0.917\n",
      "Epoch 38, loss : 0.0109, val_loss : 0.9437, acc : 0.917\n",
      "Epoch 39, loss : 0.0102, val_loss : 0.9524, acc : 0.917\n",
      "Epoch 40, loss : 0.0094, val_loss : 0.9547, acc : 0.917\n",
      "Epoch 41, loss : 0.0087, val_loss : 0.9585, acc : 0.917\n",
      "Epoch 42, loss : 0.0079, val_loss : 0.9424, acc : 0.917\n",
      "Epoch 43, loss : 0.0073, val_loss : 0.9429, acc : 0.917\n",
      "Epoch 44, loss : 0.0066, val_loss : 0.9280, acc : 0.917\n",
      "Epoch 45, loss : 0.0061, val_loss : 0.9297, acc : 0.917\n",
      "Epoch 46, loss : 0.0056, val_loss : 0.9159, acc : 0.917\n",
      "Epoch 47, loss : 0.0053, val_loss : 0.9207, acc : 0.917\n",
      "Epoch 48, loss : 0.0050, val_loss : 0.9141, acc : 0.917\n",
      "Epoch 49, loss : 0.0048, val_loss : 0.9087, acc : 0.917\n",
      "Epoch 50, loss : 0.0046, val_loss : 0.9093, acc : 0.917\n",
      "Epoch 51, loss : 0.0044, val_loss : 0.9072, acc : 0.917\n",
      "Epoch 52, loss : 0.0043, val_loss : 0.9092, acc : 0.917\n",
      "Epoch 53, loss : 0.0041, val_loss : 0.9079, acc : 0.917\n",
      "Epoch 54, loss : 0.0040, val_loss : 0.9096, acc : 0.917\n",
      "Epoch 55, loss : 0.0038, val_loss : 0.9085, acc : 0.917\n",
      "Epoch 56, loss : 0.0037, val_loss : 0.9098, acc : 0.917\n",
      "Epoch 57, loss : 0.0036, val_loss : 0.9088, acc : 0.917\n",
      "Epoch 58, loss : 0.0035, val_loss : 0.9096, acc : 0.917\n",
      "Epoch 59, loss : 0.0033, val_loss : 0.9085, acc : 0.917\n",
      "Epoch 60, loss : 0.0032, val_loss : 0.9090, acc : 0.917\n",
      "Epoch 61, loss : 0.0031, val_loss : 0.9083, acc : 0.917\n",
      "Epoch 62, loss : 0.0030, val_loss : 0.9080, acc : 0.917\n",
      "Epoch 63, loss : 0.0029, val_loss : 0.9069, acc : 0.917\n",
      "Epoch 64, loss : 0.0028, val_loss : 0.9072, acc : 0.917\n",
      "Epoch 65, loss : 0.0027, val_loss : 0.8996, acc : 0.917\n",
      "Epoch 66, loss : 0.0026, val_loss : 0.9021, acc : 0.917\n",
      "Epoch 67, loss : 0.0026, val_loss : 0.8959, acc : 0.917\n",
      "Epoch 68, loss : 0.0025, val_loss : 0.8987, acc : 0.917\n",
      "Epoch 69, loss : 0.0024, val_loss : 0.8925, acc : 0.917\n",
      "Epoch 70, loss : 0.0023, val_loss : 0.8951, acc : 0.917\n",
      "Epoch 71, loss : 0.0022, val_loss : 0.8885, acc : 0.917\n",
      "Epoch 72, loss : 0.0022, val_loss : 0.8913, acc : 0.917\n",
      "Epoch 73, loss : 0.0021, val_loss : 0.8841, acc : 0.917\n",
      "Epoch 74, loss : 0.0020, val_loss : 0.8870, acc : 0.917\n",
      "Epoch 75, loss : 0.0020, val_loss : 0.8789, acc : 0.917\n",
      "Epoch 76, loss : 0.0019, val_loss : 0.8825, acc : 0.917\n",
      "Epoch 77, loss : 0.0018, val_loss : 0.8736, acc : 0.917\n",
      "Epoch 78, loss : 0.0018, val_loss : 0.8744, acc : 0.917\n",
      "Epoch 79, loss : 0.0017, val_loss : 0.8662, acc : 0.917\n",
      "Epoch 80, loss : 0.0016, val_loss : 0.8685, acc : 0.917\n",
      "Epoch 81, loss : 0.0016, val_loss : 0.8595, acc : 0.917\n",
      "Epoch 82, loss : 0.0015, val_loss : 0.8632, acc : 0.917\n",
      "Epoch 83, loss : 0.0015, val_loss : 0.8526, acc : 0.917\n",
      "Epoch 84, loss : 0.0014, val_loss : 0.8579, acc : 0.917\n",
      "Epoch 85, loss : 0.0014, val_loss : 0.8456, acc : 0.917\n",
      "Epoch 86, loss : 0.0013, val_loss : 0.8499, acc : 0.917\n",
      "Epoch 87, loss : 0.0013, val_loss : 0.8376, acc : 0.917\n",
      "Epoch 88, loss : 0.0012, val_loss : 0.8436, acc : 0.917\n",
      "Epoch 89, loss : 0.0012, val_loss : 0.8301, acc : 0.917\n",
      "Epoch 90, loss : 0.0012, val_loss : 0.8376, acc : 0.917\n",
      "Epoch 91, loss : 0.0012, val_loss : 0.8229, acc : 0.917\n",
      "Epoch 92, loss : 0.0011, val_loss : 0.8314, acc : 0.917\n",
      "Epoch 93, loss : 0.0011, val_loss : 0.8160, acc : 0.917\n",
      "Epoch 94, loss : 0.0010, val_loss : 0.8231, acc : 0.917\n",
      "Epoch 95, loss : 0.0010, val_loss : 0.8088, acc : 0.917\n",
      "Epoch 96, loss : 0.0009, val_loss : 0.8162, acc : 0.917\n",
      "Epoch 97, loss : 0.0009, val_loss : 0.8023, acc : 0.917\n",
      "Epoch 98, loss : 0.0009, val_loss : 0.8098, acc : 0.917\n",
      "Epoch 99, loss : 0.0009, val_loss : 0.7963, acc : 0.917\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "#      # summaryの設定\n",
    "#     tf.summary.scalar('cross_entropy', loss_op)\n",
    "#     summary_op = tf.summary.merge_all()\n",
    "#     summary_writer = tf.summary.FileWriter('data', graph=sess.graph)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        \n",
    "#         #tf.summary.FileWriter('iris_sigmoid', sess.graph)\n",
    "#         summary_str = sess.run(summary_op, feed_dict={X: X_val, Y: y_val})\n",
    "#         summary_writer.add_summary(summary_str, epoch)\n",
    "        \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    \n",
    "#     # tensorboardでスカラを表示させるために必要\n",
    "#     summary_writer.flush()\n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】House Pricesのモデルを作成\n",
    "回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。\n",
    "\n",
    "\n",
    "[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使ってください。説明変数はさらに増やしても構いません。\n",
    "\n",
    "\n",
    "分類問題と回帰問題の違いを考慮してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "wHhw9ZZOR3dA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "df = pd.read_csv('train.csv')\n",
    "# データフレームから条件抽出\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "\n",
    "# NumPy 配列に変換\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y[:, np.newaxis] # (n_samples, 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.24769432],\n",
       "       [12.10901093],\n",
       "       [12.31716669],\n",
       "       ...,\n",
       "       [12.49312952],\n",
       "       [11.86446223],\n",
       "       [11.90158345]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yに対して対数変換を行う\n",
    "\n",
    "y = np.log(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 2)\n",
      "(292, 2)\n",
      "(1168, 1)\n",
      "(292, 1)\n"
     ]
    }
   ],
   "source": [
    "# 訓練データ80%、検証データ20%用に分割\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化を行う\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(934, 2)\n",
      "(234, 2)\n",
      "(934, 1)\n",
      "(234, 1)\n"
     ]
    }
   ],
   "source": [
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_std, y_train, test_size=0.2, random_state=0)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "# n_classes = 3 # 3値以上分類用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    回帰問題\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "# loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits)) # 2値分類用\n",
    "# loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits)) # 3値以上分類用\n",
    "loss_op = tf.losses.mean_squared_error(Y, logits) # 平均二乗誤差（Mean Squared Error, MSE）\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "# correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5)) # 2値分類用\n",
    "# correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) # 3値以上分類用\n",
    "correct_pred = logits\n",
    "\n",
    "# 指標値計算\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) # 分類問題用\n",
    "accuracy = tf.reduce_mean(tf.square(correct_pred - Y)) # RMSE\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 11.4022, val_loss : 19.7024, acc : 19.702\n",
      "Epoch 1, loss : 1.0593, val_loss : 6.8762, acc : 6.876\n",
      "Epoch 2, loss : 0.5319, val_loss : 4.3998, acc : 4.400\n",
      "Epoch 3, loss : 0.3653, val_loss : 3.2725, acc : 3.272\n",
      "Epoch 4, loss : 0.2800, val_loss : 2.6316, acc : 2.632\n",
      "Epoch 5, loss : 0.2280, val_loss : 2.2450, acc : 2.245\n",
      "Epoch 6, loss : 0.1927, val_loss : 1.9976, acc : 1.998\n",
      "Epoch 7, loss : 0.1668, val_loss : 1.8074, acc : 1.807\n",
      "Epoch 8, loss : 0.1458, val_loss : 1.6507, acc : 1.651\n",
      "Epoch 9, loss : 0.1277, val_loss : 1.5150, acc : 1.515\n",
      "Epoch 10, loss : 0.1127, val_loss : 1.3961, acc : 1.396\n",
      "Epoch 11, loss : 0.1000, val_loss : 1.2936, acc : 1.294\n",
      "Epoch 12, loss : 0.0900, val_loss : 1.2067, acc : 1.207\n",
      "Epoch 13, loss : 0.0817, val_loss : 1.1486, acc : 1.149\n",
      "Epoch 14, loss : 0.0750, val_loss : 1.0893, acc : 1.089\n",
      "Epoch 15, loss : 0.0694, val_loss : 1.0298, acc : 1.030\n",
      "Epoch 16, loss : 0.0649, val_loss : 0.9770, acc : 0.977\n",
      "Epoch 17, loss : 0.0610, val_loss : 0.9255, acc : 0.925\n",
      "Epoch 18, loss : 0.0577, val_loss : 0.8800, acc : 0.880\n",
      "Epoch 19, loss : 0.0548, val_loss : 0.8389, acc : 0.839\n",
      "Epoch 20, loss : 0.0520, val_loss : 0.7925, acc : 0.792\n",
      "Epoch 21, loss : 0.0490, val_loss : 0.7631, acc : 0.763\n",
      "Epoch 22, loss : 0.0470, val_loss : 0.7484, acc : 0.748\n",
      "Epoch 23, loss : 0.0448, val_loss : 0.7227, acc : 0.723\n",
      "Epoch 24, loss : 0.0427, val_loss : 0.6898, acc : 0.690\n",
      "Epoch 25, loss : 0.0409, val_loss : 0.6801, acc : 0.680\n",
      "Epoch 26, loss : 0.0398, val_loss : 0.6393, acc : 0.639\n",
      "Epoch 27, loss : 0.0382, val_loss : 0.6234, acc : 0.623\n",
      "Epoch 28, loss : 0.0370, val_loss : 0.5806, acc : 0.581\n",
      "Epoch 29, loss : 0.0359, val_loss : 0.5520, acc : 0.552\n",
      "Epoch 30, loss : 0.0351, val_loss : 0.5463, acc : 0.546\n",
      "Epoch 31, loss : 0.0350, val_loss : 0.5114, acc : 0.511\n",
      "Epoch 32, loss : 0.0341, val_loss : 0.4997, acc : 0.500\n",
      "Epoch 33, loss : 0.0342, val_loss : 0.4900, acc : 0.490\n",
      "Epoch 34, loss : 0.0333, val_loss : 0.4792, acc : 0.479\n",
      "Epoch 35, loss : 0.0330, val_loss : 0.4284, acc : 0.428\n",
      "Epoch 36, loss : 0.0321, val_loss : 0.4267, acc : 0.427\n",
      "Epoch 37, loss : 0.0320, val_loss : 0.4367, acc : 0.437\n",
      "Epoch 38, loss : 0.0326, val_loss : 0.4055, acc : 0.405\n",
      "Epoch 39, loss : 0.0312, val_loss : 0.4013, acc : 0.401\n",
      "Epoch 40, loss : 0.0321, val_loss : 0.4053, acc : 0.405\n",
      "Epoch 41, loss : 0.0330, val_loss : 0.3588, acc : 0.359\n",
      "Epoch 42, loss : 0.0306, val_loss : 0.3641, acc : 0.364\n",
      "Epoch 43, loss : 0.0309, val_loss : 0.3999, acc : 0.400\n",
      "Epoch 44, loss : 0.0351, val_loss : 0.3930, acc : 0.393\n",
      "Epoch 45, loss : 0.0342, val_loss : 0.4088, acc : 0.409\n",
      "Epoch 46, loss : 0.0348, val_loss : 0.4284, acc : 0.428\n",
      "Epoch 47, loss : 0.0360, val_loss : 0.4313, acc : 0.431\n",
      "Epoch 48, loss : 0.0344, val_loss : 0.4193, acc : 0.419\n",
      "Epoch 49, loss : 0.0341, val_loss : 0.3841, acc : 0.384\n",
      "Epoch 50, loss : 0.0307, val_loss : 0.3453, acc : 0.345\n",
      "Epoch 51, loss : 0.0296, val_loss : 0.3188, acc : 0.319\n",
      "Epoch 52, loss : 0.0269, val_loss : 0.2982, acc : 0.298\n",
      "Epoch 53, loss : 0.0262, val_loss : 0.2947, acc : 0.295\n",
      "Epoch 54, loss : 0.0271, val_loss : 0.2883, acc : 0.288\n",
      "Epoch 55, loss : 0.0265, val_loss : 0.2792, acc : 0.279\n",
      "Epoch 56, loss : 0.0271, val_loss : 0.2637, acc : 0.264\n",
      "Epoch 57, loss : 0.0280, val_loss : 0.2611, acc : 0.261\n",
      "Epoch 58, loss : 0.0259, val_loss : 0.2995, acc : 0.299\n",
      "Epoch 59, loss : 0.0337, val_loss : 0.6110, acc : 0.611\n",
      "Epoch 60, loss : 0.0464, val_loss : 0.6576, acc : 0.658\n",
      "Epoch 61, loss : 0.0421, val_loss : 0.3952, acc : 0.395\n",
      "Epoch 62, loss : 0.0336, val_loss : 0.3204, acc : 0.320\n",
      "Epoch 63, loss : 0.0278, val_loss : 0.3537, acc : 0.354\n",
      "Epoch 64, loss : 0.0251, val_loss : 0.3359, acc : 0.336\n",
      "Epoch 65, loss : 0.0246, val_loss : 0.3157, acc : 0.316\n",
      "Epoch 66, loss : 0.0343, val_loss : 0.4979, acc : 0.498\n",
      "Epoch 67, loss : 0.0462, val_loss : 0.6312, acc : 0.631\n",
      "Epoch 68, loss : 0.0517, val_loss : 0.6021, acc : 0.602\n",
      "Epoch 69, loss : 0.0347, val_loss : 0.2898, acc : 0.290\n",
      "Epoch 70, loss : 0.0270, val_loss : 0.3359, acc : 0.336\n",
      "Epoch 71, loss : 0.0267, val_loss : 0.3781, acc : 0.378\n",
      "Epoch 72, loss : 0.0241, val_loss : 0.2763, acc : 0.276\n",
      "Epoch 73, loss : 0.0232, val_loss : 0.2781, acc : 0.278\n",
      "Epoch 74, loss : 0.0260, val_loss : 0.2951, acc : 0.295\n",
      "Epoch 75, loss : 0.0273, val_loss : 0.2769, acc : 0.277\n",
      "Epoch 76, loss : 0.0268, val_loss : 0.2568, acc : 0.257\n",
      "Epoch 77, loss : 0.0225, val_loss : 0.2724, acc : 0.272\n",
      "Epoch 78, loss : 0.0235, val_loss : 0.2746, acc : 0.275\n",
      "Epoch 79, loss : 0.0269, val_loss : 0.2902, acc : 0.290\n",
      "Epoch 80, loss : 0.0312, val_loss : 0.3497, acc : 0.350\n",
      "Epoch 81, loss : 0.0331, val_loss : 0.2826, acc : 0.283\n",
      "Epoch 82, loss : 0.0338, val_loss : 0.2865, acc : 0.287\n",
      "Epoch 83, loss : 0.0271, val_loss : 0.3275, acc : 0.328\n",
      "Epoch 84, loss : 0.0266, val_loss : 0.3577, acc : 0.358\n",
      "Epoch 85, loss : 0.0243, val_loss : 0.3011, acc : 0.301\n",
      "Epoch 86, loss : 0.0273, val_loss : 0.3219, acc : 0.322\n",
      "Epoch 87, loss : 0.0330, val_loss : 0.3379, acc : 0.338\n",
      "Epoch 88, loss : 0.0314, val_loss : 0.3022, acc : 0.302\n",
      "Epoch 89, loss : 0.0271, val_loss : 0.2384, acc : 0.238\n",
      "Epoch 90, loss : 0.0210, val_loss : 0.2123, acc : 0.212\n",
      "Epoch 91, loss : 0.0206, val_loss : 0.2728, acc : 0.273\n",
      "Epoch 92, loss : 0.0232, val_loss : 0.2748, acc : 0.275\n",
      "Epoch 93, loss : 0.0246, val_loss : 0.2642, acc : 0.264\n",
      "Epoch 94, loss : 0.0288, val_loss : 0.2704, acc : 0.270\n",
      "Epoch 95, loss : 0.0280, val_loss : 0.2466, acc : 0.247\n",
      "Epoch 96, loss : 0.0272, val_loss : 0.2291, acc : 0.229\n",
      "Epoch 97, loss : 0.0221, val_loss : 0.2393, acc : 0.239\n",
      "Epoch 98, loss : 0.0216, val_loss : 0.2433, acc : 0.243\n",
      "Epoch 99, loss : 0.0239, val_loss : 0.2718, acc : 0.272\n",
      "test_acc : 0.219\n"
     ]
    }
   ],
   "source": [
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "     \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "      \n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test_std, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】MNISTのモデルを作成\n",
    "ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。\n",
    "\n",
    "\n",
    "3クラス以上の分類という点ではひとつ前のIrisと同様です。入力が画像であるという点で異なります。\n",
    "\n",
    "\n",
    "スクラッチで実装したモデルの再現を目指してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab使う場合\n",
    "# !pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n",
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "uint8\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# MNISTのデータセットを読み込み\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape) # (60000, 28, 28)\n",
    "print(X_test.shape) # (10000, 28, 28)\n",
    "print(X_train[0].dtype) # uint8\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 前処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "print(X_train.shape) # (60000, 784)\n",
    "print(X_test.shape) # (10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# ワンホット処理\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.fit_transform(y_test[:, np.newaxis])\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_test_one_hot.shape) # (10000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (48000, 784)\n",
      "X_val.shape (12000, 784)\n",
      "y_train (48000, 10)\n",
      "y_val.shape (12000, 10)\n"
     ]
    }
   ],
   "source": [
    "# trainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=0)\n",
    "print('X_train.shape', X_train.shape)\n",
    "print('X_val.shape', X_val.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('y_val.shape', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    3クラス以上の分類\n",
    "    \"\"\"\n",
    "    tf.random.set_random_seed(0)\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "# loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits)) # 2値分類用\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "# correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5)) # 2値分類用\n",
    "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1)) # 最大値のインデックスを比較\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 2.6046, val_loss : 7.2459, acc : 0.773\n",
      "Epoch 1, loss : 0.4067, val_loss : 2.5514, acc : 0.756\n",
      "Epoch 2, loss : 0.1471, val_loss : 1.2032, acc : 0.741\n",
      "Epoch 3, loss : 0.0866, val_loss : 0.9010, acc : 0.796\n",
      "Epoch 4, loss : 0.0660, val_loss : 0.7807, acc : 0.832\n",
      "Epoch 5, loss : 0.0553, val_loss : 0.7139, acc : 0.844\n",
      "Epoch 6, loss : 0.0470, val_loss : 0.6484, acc : 0.871\n",
      "Epoch 7, loss : 0.0418, val_loss : 0.6069, acc : 0.886\n",
      "Epoch 8, loss : 0.0378, val_loss : 0.5729, acc : 0.894\n",
      "Epoch 9, loss : 0.0345, val_loss : 0.5525, acc : 0.899\n",
      "test_acc : 0.897\n"
     ]
    }
   ],
   "source": [
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(参考記事)[TensorFlow入門 - 四則演算と基礎的な数学関数まとめ](https://qiita.com/negabaro/items/bdaeaf6ba4bc9fc81208)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjgLuLDDKEXFi4WsTMHpbe",
   "mount_file_id": "1e9zFaiY7VkwFl3k2hI8rQYe_OWbYicnc",
   "name": "Term2_Sprint13_Framework1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "p3.7",
   "language": "python",
   "name": "p3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
